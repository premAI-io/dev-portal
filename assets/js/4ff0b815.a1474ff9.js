"use strict";(self.webpackChunkprem_docs=self.webpackChunkprem_docs||[]).push([[1256],{3905:(e,n,t)=>{t.d(n,{Zo:()=>g,kt:()=>C});var o=t(7294);function A(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){A(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function r(e,n){if(null==e)return{};var t,o,A=function(e,n){if(null==e)return{};var t,o,A={},i=Object.keys(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||(A[t]=e[t]);return A}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(A[t]=e[t])}return A}var s=o.createContext({}),l=function(e){var n=o.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):a(a({},n),e)),t},g=function(e){var n=l(e.components);return o.createElement(s.Provider,{value:n},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},d=o.forwardRef((function(e,n){var t=e.components,A=e.mdxType,i=e.originalType,s=e.parentName,g=r(e,["components","mdxType","originalType","parentName"]),p=l(t),d=A,C=p["".concat(s,".").concat(d)]||p[d]||m[d]||i;return t?o.createElement(C,a(a({ref:n},g),{},{components:t})):o.createElement(C,a({ref:n},g))}));function C(e,n){var t=arguments,A=n&&n.mdxType;if("string"==typeof e||A){var i=t.length,a=new Array(i);a[0]=d;var r={};for(var s in n)hasOwnProperty.call(n,s)&&(r[s]=n[s]);r.originalType=e,r[p]="string"==typeof e?e:A,a[1]=r;for(var l=2;l<i;l++)a[l]=t[l];return o.createElement.apply(null,a)}return o.createElement.apply(null,t)}d.displayName="MDXCreateElement"},2790:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>a,default:()=>m,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var o=t(7462),A=(t(7294),t(3905));const i={slug:"mlops-more-oops-than-ops",title:"MLOps: More Oops than Ops",authors:["biswaroop","casperdcl"],tags:["llm","prem","performance","mlops","onnx","tensorrt"],description:"Navigating the Challenges of Improving Inference Latency for New Large Models through ONNX and TensorRT Optimization.",image:"./banner.png"},a=void 0,r={permalink:"/blog/mlops-more-oops-than-ops",editUrl:"https://github.com/premAI-io/dev-portal/blob/main/blog/2023-08-16-mlops-more-oops-than-ops/index.md",source:"@site/blog/2023-08-16-mlops-more-oops-than-ops/index.md",title:"MLOps: More Oops than Ops",description:"Navigating the Challenges of Improving Inference Latency for New Large Models through ONNX and TensorRT Optimization.",date:"2023-08-16T00:00:00.000Z",formattedDate:"August 16, 2023",tags:[{label:"llm",permalink:"/blog/tags/llm"},{label:"prem",permalink:"/blog/tags/prem"},{label:"performance",permalink:"/blog/tags/performance"},{label:"mlops",permalink:"/blog/tags/mlops"},{label:"onnx",permalink:"/blog/tags/onnx"},{label:"tensorrt",permalink:"/blog/tags/tensorrt"}],readingTime:13.08,hasTruncateMarker:!0,authors:[{name:"Biswaroop Bhattacharjee",title:"Core contributor @ PremAI",url:"https://github.com/biswaroop1547",imageURL:"https://github.com/biswaroop1547.png",key:"biswaroop"},{name:"Casper da Costa-Luis",title:"Core contributor @ PremAI",url:"https://github.com/casperdcl",imageURL:"https://github.com/casperdcl.png",key:"casperdcl"}],frontMatter:{slug:"mlops-more-oops-than-ops",title:"MLOps: More Oops than Ops",authors:["biswaroop","casperdcl"],tags:["llm","prem","performance","mlops","onnx","tensorrt"],description:"Navigating the Challenges of Improving Inference Latency for New Large Models through ONNX and TensorRT Optimization.",image:"./banner.png"},prevItem:{title:"Evaluating Open-Source Large Language Models",permalink:"/blog/evaluating-open-source-llms"},nextItem:{title:"Teach a Q&A Chatbot with Audio Recordings",permalink:"/blog/teach-chatbot-with-audio"}},s={image:t(6037).Z,authorsImageUrls:[void 0,void 0]},l=[{value:"The Promise of Faster Inference",id:"the-promise-of-faster-inference",level:2},{value:"Model Conversion Errors",id:"model-conversion-errors",level:2},{value:"<code>torch.onnx.export</code> gibberish text",id:"torchonnxexport-gibberish-text",level:3},{value:"<code>optimum-cli</code> gibberish text and <code>tensorrt</code> slowness",id:"optimum-cli-gibberish-text-and-tensorrt-slowness",level:3},{value:"<code>optimum-cli</code> segfaults",id:"optimum-cli-segfaults",level:3},{value:"<code>torch.onnx.export</code> gibberish images",id:"torchonnxexport-gibberish-images",level:3},{value:"Other Frustrations",id:"other-frustrations",level:2},{value:"Conclusion",id:"conclusion",level:2}],g={toc:l},p="wrapper";function m(e){let{components:n,...i}=e;return(0,A.kt)(p,(0,o.Z)({},g,i,{components:n,mdxType:"MDXLayout"}),(0,A.kt)("p",null,(0,A.kt)("img",{alt:"Banner",src:t(6037).Z,width:"512",height:"512"}),(0,A.kt)("br",null),"\n\ud83e\udd16 ",(0,A.kt)("em",{parentName:"p"},"image generated using the ",(0,A.kt)("a",{parentName:"em",href:"https://registry.premai.io/detail.html?service=stable-diffusion-2-1"},"Stable Diffusion 2.1")," model mentioned in this post")),(0,A.kt)("head",null,(0,A.kt)("meta",{name:"twitter:image",content:"./banner.png"})),(0,A.kt)("p",null,"As model complexity increases exponentially, so too does the need for effective MLOps practices. This post acts as a transparent write-up of all the MLOps frustrations I\u2019ve experienced in the last few days. By sharing my challenges and insights, I hope to contribute to a community that openly discusses and shares solutions for MLOps challenges."),(0,A.kt)("p",null,"My goal was to improve Inference latency of few of the current state-of-the-art LLMs."),(0,A.kt)("p",null,"Unfortunately, simply downloading trained model weights & existing code doesn't solve this problem."),(0,A.kt)("h2",{id:"the-promise-of-faster-inference"},"The Promise of Faster Inference"),(0,A.kt)("p",null,"My first target here was ",(0,A.kt)("a",{parentName:"p",href:"http://registry.premai.io/detail.html?service=llama-2-7b-chat"},"Llama 2"),". I wanted to convert it into ",(0,A.kt)("a",{parentName:"p",href:"https://onnx.ai"},"ONNX")," format, which could then be converted to ",(0,A.kt)("a",{parentName:"p",href:"https://developer.nvidia.com/tensorrt-getting-started"},"TensorRT"),", and finally served using ",(0,A.kt)("a",{parentName:"p",href:"https://developer.nvidia.com/triton-inference-server"},"Triton Inference Server"),"."),(0,A.kt)("p",null,"TensorRT optimizes the model network by combining layers and optimizing kernel selection for improved latency, throughput, power efficiency and memory consumption. If the application specifies, it will additionally optimize the network to run in lower precision, further increasing performance and reducing memory requirements."),(0,A.kt)("p",null,"From online benchmarks [",(0,A.kt)("a",{parentName:"p",href:"https://github.com/kentaroy47/benchmark-FP32-FP16-INT8-with-TensorRT"},"1"),", ",(0,A.kt)("a",{parentName:"p",href:"https://medium.com/@abhismatrix/speeding-deep-learning-inference-by-upto-20x-6c0c0f6fba81"},"2"),"] it seems possible to achieve a 2~3x boost to latency (by reducing precision without hurting quality much). But the workings for these kind of format conversions feel super flaky, things break too often (without any solution to be found online). Yes, it\u2019s somewhat expected since these models are so new, with different architectures using different (not yet widely-supported) layers and operators."),(0,A.kt)("h2",{id:"model-conversion-errors"},"Model Conversion Errors"),(0,A.kt)("p",null,"Let\u2019s start with ",(0,A.kt)("strong",{parentName:"p"},"Llama 2 7B chat"),","),(0,A.kt)("ol",null,(0,A.kt)("li",{parentName:"ol"},"Firstly I\u2019ve downloaded Llama-2-7B-Chat weights from Meta\u2019s Official repository ",(0,A.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/llama"},"here")," after requesting."),(0,A.kt)("li",{parentName:"ol"},"Convert raw weights to huggingface format using ",(0,A.kt)("a",{parentName:"li",href:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py"},"this")," script by Huggingface. Let\u2019s say we save it under ",(0,A.kt)("inlineCode",{parentName:"li"},"llama-2-7b-chat-hf")," directory locally.")),(0,A.kt)("p",null,"Now I considered two options for converting huggingface models to ONNX format:"),(0,A.kt)("h3",{id:"torchonnxexport-gibberish-text"},(0,A.kt)("inlineCode",{parentName:"h3"},"torch.onnx.export")," gibberish text"),(0,A.kt)("p",null,"Let\u2019s write an ",(0,A.kt)("inlineCode",{parentName:"p"},"export_to_onnx")," function which will load the tokenizer & model, and export it into ONNX format:"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-python"},'import torch\nfrom composer.utils import parse_uri, reproducibility\nfrom pathlib import Path\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n\ndef export_to_onnx(\n    pretrained_model_name_or_path: str,\n    output_folder: str,\n    verify_export: bool,\n    max_seq_len: int | None = None,\n):\n    reproducibility.seed_all(42)\n    _, _, parsed_save_path = parse_uri(output_folder)\n    # Load HF config/model/tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, use_fast=True)\n    config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n    if hasattr(config, \'attn_config\'):\n        config.attn_config[\'attn_impl\'] = \'torch\'\n\n    model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, config=config).to("cuda:0")\n    model.eval()\n    # tips: https://huggingface.co/docs/transformers/v4.31.0/en/model_doc/llama2\n    tokenizer.add_special_tokens({"pad_token": "<pad>"})\n    model.resize_token_embeddings(len(tokenizer))\n    model.config.pad_token_id = tokenizer.pad_token_id\n    sample_input = tokenizer(\n        "Hello, my dog is cute",\n        padding="max_length",\n        max_length=max_seq_len or model.config.max_seq_len,\n        truncation=True,\n        return_tensors="pt",\n        add_special_tokens=True).to("cuda:0")\n\n    with torch.no_grad():\n        model(**sample_input)\n\n    output_file = Path(parsed_save_path) / \'model.onnx\'\n    output_file.parent.mkdir(parents=True, exist_ok=True)\n    # Put sample input on cpu for export\n    sample_input = {k: v.cpu() for k, v in sample_input.items()}\n    model = model.to("cpu")\n    torch.onnx.export(\n        model,\n        (sample_input,),\n        str(output_file),\n        input_names=[\'input_ids\', \'attention_mask\'],\n        output_names=[\'output\'],\n        opset_version=16)\n')),(0,A.kt)("p",null,"We can also check if the exported & original models' outputs are similar:"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-python"},"# (Optional) verify onnx model outputs\nimport onnx\nimport onnx.checker\nimport onnxruntime as ort\n\nwith torch.no_grad():\n    orig_out = model(**sample_input)\n    orig_out.logits = orig_out.logits.cpu()  # put on cpu for export\n\n_ = onnx.load(str(output_file))\nonnx.checker.check_model(str(output_file))\nort_session = ort.InferenceSession(str(output_file))\nfor key, value in sample_input.items():\n    sample_input[key] = value.cpu().numpy()\nloaded_model_out = ort_session.run(None, sample_input)\ntorch.testing.assert_close(\n    orig_out.logits.detach().numpy(),\n    loaded_model_out[0],\n    rtol=1e-2,\n    atol=1e-2,\n    msg=f'output mismatch between the orig and onnx exported model')\nprint('Success: exported & original model outputs match')\n")),(0,A.kt)("p",null,"Assuming we've saved the ONNX model in ",(0,A.kt)("inlineCode",{parentName:"p"},"./llama-2-7b-onnx/"),", we can now run inference using ",(0,A.kt)("inlineCode",{parentName:"p"},"onnxruntime"),":"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-python"},'import onnx\nimport onnx.checker\nimport onnxruntime as ort\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\noutput_file = \'llama-2-7b-onnx/model.onnx\'  # converted model from above\nort_session = ort.InferenceSession(str(output_file))\ntokenizer = AutoTokenizer.from_pretrained("llama-2-7b-chat-hf", use_fast=True)\ntokenizer.add_special_tokens({"pad_token": "<pad>"})\ninputs = tokenizer(\n    "Hello, my dog is cute",\n    padding="max_length",\n    max_length=1024,\n    truncation=True,\n    return_tensors="np",\n    add_special_tokens=True)\nloaded_model_out = ort_session.run(None, inputs.data)\ntokenizer.batch_decode(torch.argmax(torch.tensor(loaded_model_out[0]), dim=-1))\n')),(0,A.kt)("p",null,"\ud83d\ude16 On my machine, this generates really funky outputs:"),(0,A.kt)("p",null,(0,A.kt)("inlineCode",{parentName:"p"},"\u0409\u0409\u0409\u0409\u0409\u0409\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Hello Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis..........SMSMSMSMSMSMSMSMSMSMSMS Unterscheidung, I name is ough,")),(0,A.kt)("p",null,"... which is mostly due to missing a proper decoding strategy (",(0,A.kt)("a",{parentName:"p",href:"https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p#1-pick-the-top-token-greedy-decoding"},"greedy"),", ",(0,A.kt)("a",{parentName:"p",href:"https://machinelearningmastery.com/beam-search-decoder-natural-language-processing"},"beam"),", etc.) while generating tokens."),(0,A.kt)("h3",{id:"optimum-cli-gibberish-text-and-tensorrt-slowness"},(0,A.kt)("inlineCode",{parentName:"h3"},"optimum-cli")," gibberish text and ",(0,A.kt)("inlineCode",{parentName:"h3"},"tensorrt")," slowness"),(0,A.kt)("p",null,"To solve the problem above, we can try a different exporter which includes decoding strategies."),(0,A.kt)("p",null,"Using the ",(0,A.kt)("a",{parentName:"p",href:"https://huggingface.co/docs/transformers/serialization#export-to-onnx"},"Optimum ONNX exporter")," instead (assuming the original model is in ",(0,A.kt)("inlineCode",{parentName:"p"},"./llama-2-7b-chat-hf/"),"), we can do:"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-sh"},"optimum-cli export onnx \\\n  --model ./llama-2-7b-chat-hf/ --task text-generation --framework pt \\\n  --opset 16 --sequence_length 1024 --batch_size 1 --device cuda --fp16 \\\n  llama-2-7b-optimum/\n")),(0,A.kt)("p",null,"\u231b This takes a few minutes to generate. If you don\u2019t has a GPU for this conversion, then remove ",(0,A.kt)("inlineCode",{parentName:"p"},"--device cuda")," from the above command."),(0,A.kt)("p",null,"The result is:"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre"},"llama-2-7b-optimum\n \u251c\u2500\u2500 config.json\n \u251c\u2500\u2500 Constant_162_attr__value\n \u251c\u2500\u2500 Constant_170_attr__value\n \u251c\u2500\u2500 decoder_model.onnx\n \u251c\u2500\u2500 decoder_model.onnx_data\n \u251c\u2500\u2500 generation_config.json\n \u251c\u2500\u2500 special_tokens_map.json\n \u251c\u2500\u2500 tokenizer_config.json\n \u251c\u2500\u2500 tokenizer.json\n \u2514\u2500\u2500 tokenizer.model\n")),(0,A.kt)("p",null,"Now when I try to do inference using ",(0,A.kt)("inlineCode",{parentName:"p"},"optimum.onnxruntime.ORTModelForCausalLM"),", things work fine (though slowly) using the ",(0,A.kt)("inlineCode",{parentName:"p"},"CPUExecutionProvider"),":"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-python"},'from transformers import AutoTokenizer\nfrom optimum.onnxruntime import ORTModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained("./onnx_optimum")\nmodel = ORTModelForCausalLM.from_pretrained("./onnx_optimum/", use_cache=False, use_io_binding=False)\ninputs = tokenizer("My name is Arthur and I live in", return_tensors="pt")\ngen_tokens = model.generate(**inputs, max_length=16)\nassert model.providers == [\'CPUExecutionProvider\']\nprint(tokenizer.batch_decode(gen_tokens))\n')),(0,A.kt)("p",null,"After waiting a long time, we get a result:"),(0,A.kt)("p",null,(0,A.kt)("inlineCode",{parentName:"p"},"<s> My name is Arthur and I live in a small town in the countr")),(0,A.kt)("p",null,"But when switching to the faster ",(0,A.kt)("inlineCode",{parentName:"p"},"CUDAExecutionProvider"),", I get gibberish text on inference:"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-python"},'model = ORTModelForCausalLM.from_pretrained("./onnx_optimum/", use_cache=False, use_io_binding=False, provider="CUDAExecutionProvider")\ninputs = tokenizer("My name is Arthur and I live in", return_tensors="pt").to("cuda")\ngen_tokens = model.generate(**inputs, max_length=16)\nassert model.providers == [\'CUDAExecutionProvider\', \'CPUExecutionProvider\']\nprint(tokenizer.batch_decode(gen_tokens))\n')),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-json"},"2023-08-02 19:47:43.534099146 [W:onnxruntime:, session_state.cc:1169 VerifyEachNodeIsAssignedToAnEp]\nSome nodes were not assigned to the preferred execution providers which may or may not\nhave an negative impact on performance. e.g. ORT explicitly assigns shape related ops\nto CPU to improve perf.\n2023-08-02 19:47:43.534136078 [W:onnxruntime:, session_state.cc:1171 VerifyEachNodeIsAssignedToAnEp]\nRerunning with verbose output on a non-minimal build will show node assignments.\n\n<s> My name is Arthur and I live in a<unk><unk><unk><unk><unk><unk>\n")),(0,A.kt)("p",null,"Even with different ",(0,A.kt)("inlineCode",{parentName:"p"},"temperature")," and other parameter values, it always yields unintelligible outputs, as reported in ",(0,A.kt)("a",{parentName:"p",href:"https://github.com/huggingface/optimum/issues/1248"},"optimum#1248"),"."),(0,A.kt)("p",null,"\ud83c\udf89 ",(0,A.kt)("strong",{parentName:"p"},"Update"),": after about a week this issue seemed to magically disappear \u2014 possibly due to a new version of ",(0,A.kt)("inlineCode",{parentName:"p"},"llama-2-7b-chat-hf")," being released."),(0,A.kt)("p",null,"Using the new model with ",(0,A.kt)("inlineCode",{parentName:"p"},"max_length=128"),", :"),(0,A.kt)("ul",null,(0,A.kt)("li",{parentName:"ul"},"Prompt: ",(0,A.kt)("em",{parentName:"li"},"Why should one run Machine learning model on-premises?"),(0,A.kt)("ul",{parentName:"li"},(0,A.kt)("li",{parentName:"ul"},"ONNX inference latency: ",(0,A.kt)("inlineCode",{parentName:"li"},"2.31s")),(0,A.kt)("li",{parentName:"ul"},"HuggingFace version latency: ",(0,A.kt)("inlineCode",{parentName:"li"},"3s"))))),(0,A.kt)("p",null,"\ud83d\ude80 The ONNX model is ~23% faster than the HuggingFace variant!"),(0,A.kt)("p",null,"\u26a0\ufe0f However, while both CPU and CUDA providers work, there now seems to be a bug when trying ",(0,A.kt)("inlineCode",{parentName:"p"},"TensorrtExecutionProvider")," \u2014 reported in ",(0,A.kt)("a",{parentName:"p",href:"https://github.com/huggingface/optimum/issues/1278"},"optimum#1278"),"."),(0,A.kt)("h3",{id:"optimum-cli-segfaults"},(0,A.kt)("inlineCode",{parentName:"h3"},"optimum-cli")," segfaults"),(0,A.kt)("p",null,"Next let\u2019s try with the ",(0,A.kt)("a",{parentName:"p",href:"https://huggingface.co/databricks/dolly-v2-7b"},(0,A.kt)("strong",{parentName:"a"},"Dolly-v2 7B"))," from ",(0,A.kt)("a",{parentName:"p",href:"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm"},"Databricks"),". The equivalent ",(0,A.kt)("inlineCode",{parentName:"p"},"optimum-cli")," command for ONNX conversion would be:"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-sh"},"optimum-cli export onnx \\\n  --model 'databricks/dolly-v2-7b' --task text-generation --framework pt \\\n  --opset 17 --sequence_length 1024 --batch_size 1 --fp16 --device cuda \\\n  dolly_optimum\n")),(0,A.kt)("p",null,"\ud83d\ude22 It uses around 17GB of my GPU RAM, seemingly working fine but finally ending with a segmentation fault:"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-json"},"======= Diagnostic Run torch.onnx.export version 2.1.0.dev20230804+cu118 =======\nverbose: False, log level: 40\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\nSaving external data to one file...\n2023-08-09 20:59:33.334484259 [W:onnxruntime:, session_state.cc:1169 VerifyEachNodeIsAssignedToAnEp]\nSome nodes were not assigned to the preferred execution providers which may or may not\nhave an negative impact on performance. e.g. ORT explicitly assigns shape related ops\nto CPU to improve perf.\n2023-08-09 20:59:33.334531829 [W:onnxruntime:, session_state.cc:1171 VerifyEachNodeIsAssignedToAnEp]\nRerunning with verbose output on a non-minimal build will show node assignments.\nAsked a sequence length of 1024, but a sequence length of 1 will be used with\nuse_past == True for `input_ids`.\nPost-processing the exported models...\nSegmentation fault (core dumped)\n")),(0,A.kt)("p",null,"Confusingly, despite this error, all model files seem to be converted and saved to disk. Other people have reported similar segfault issues while exporting (",(0,A.kt)("a",{parentName:"p",href:"https://github.com/huggingface/transformers/issues/21360"},"transformers#21360"),", ",(0,A.kt)("a",{parentName:"p",href:"https://github.com/huggingface/optimum/issues/798"},"optimum#798"),")."),(0,A.kt)("p",null,"Results using the Dolly v2 model:"),(0,A.kt)("ul",null,(0,A.kt)("li",{parentName:"ul"},"Prompt: ",(0,A.kt)("em",{parentName:"li"},"Why should one run Machine learning model on-premises?"),(0,A.kt)("ul",{parentName:"li"},(0,A.kt)("li",{parentName:"ul"},"ONNX inference latency: ",(0,A.kt)("inlineCode",{parentName:"li"},"8.2s")),(0,A.kt)("li",{parentName:"ul"},"HuggingFace version latency: ",(0,A.kt)("inlineCode",{parentName:"li"},"5.2s"))))),(0,A.kt)("p",null,"\ud83d\ude20 The ONNX model is actually ~58% slower than the HuggingFace variant!"),(0,A.kt)("p",null,"To make things faster, we can try to optimize the model:"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-sh"},"optimum-cli onnxruntime optimize -O4 --onnx_model ./dolly_optimum/ -o dolly_optimized/\n")),(0,A.kt)("p",null,"The ",(0,A.kt)("a",{parentName:"p",href:"https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/optimization"},"different optimization levels")," are:"),(0,A.kt)("ul",null,(0,A.kt)("li",{parentName:"ul"},(0,A.kt)("inlineCode",{parentName:"li"},"-O1"),": basic general optimizations."),(0,A.kt)("li",{parentName:"ul"},(0,A.kt)("inlineCode",{parentName:"li"},"-O2"),": basic and extended general optimizations, transformers-specific fusions."),(0,A.kt)("li",{parentName:"ul"},(0,A.kt)("inlineCode",{parentName:"li"},"-O3"),": same as O2 with GELU approximation."),(0,A.kt)("li",{parentName:"ul"},(0,A.kt)("inlineCode",{parentName:"li"},"-O4"),": same as O3 with mixed precision (fp16, GPU-only).")),(0,A.kt)("p",null,"We still get the same segfault error for all of the levels."),(0,A.kt)("p",null,"For ",(0,A.kt)("inlineCode",{parentName:"p"},"-O1"),", the model gets saved but there\u2019s no noticeable performance change. For ",(0,A.kt)("inlineCode",{parentName:"p"},"-O2")," it gets killed (even though I have 40GB A100 GPU + 80GB CPU RAM). Meanwhile for ",(0,A.kt)("inlineCode",{parentName:"p"},"-O3")," & ",(0,A.kt)("inlineCode",{parentName:"p"},"-O4")," it gives seg-fault (above) while only partially saving the model files."),(0,A.kt)("h3",{id:"torchonnxexport-gibberish-images"},(0,A.kt)("inlineCode",{parentName:"h3"},"torch.onnx.export")," gibberish images"),(0,A.kt)("p",null,"Moving on from text-based models, let\u2019s now look at an image generator. We can try to speed up the ",(0,A.kt)("a",{parentName:"p",href:"https://huggingface.co/stabilityai/stable-diffusion-2-1"},(0,A.kt)("strong",{parentName:"a"},"Stable Diffusion 2.1"))," model. In an IPython shell:"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-python"},'from diffusers import StableDiffusionPipeline\npipe = StableDiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16).to("cuda:0")\n%time img = pipe("Iron man laughing", num_inference_steps=20, num_images_per_prompt=1).images[0]\nimg.save("iron_man.png", format="PNG")\n')),(0,A.kt)("p",null,"The latency (as measured by the ",(0,A.kt)("inlineCode",{parentName:"p"},"%time")," magic) is ",(0,A.kt)("inlineCode",{parentName:"p"},"3.25 s"),"."),(0,A.kt)("p",null,"To convert to ONNX format, we can use ",(0,A.kt)("a",{parentName:"p",href:"https://github.com/huggingface/diffusers/blob/main/scripts/convert_stable_diffusion_checkpoint_to_onnx.py"},"this script"),":"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-sh"},"python convert_stable_diffusion_checkpoint_to_onnx.py \\\n  --model_path stabilityai/stable-diffusion-2-1 \\\n  --output_path sd_onnx/ --opset 16 --fp16\n")),(0,A.kt)("blockquote",null,(0,A.kt)("p",{parentName:"blockquote"},"\u2139\ufe0f Note: if a model uses operators unsupported by the ",(0,A.kt)("inlineCode",{parentName:"p"},"opset")," number above, you'll have to upgrade ",(0,A.kt)("inlineCode",{parentName:"p"},"pytorch")," to the nightly build:"),(0,A.kt)("pre",{parentName:"blockquote"},(0,A.kt)("code",{parentName:"pre",className:"language-sh"},"pip uninstall torch\npip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121\n"))),(0,A.kt)("p",null,"The result is:"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre"},"sd_onnx/\n\u251c\u2500\u2500 model_index.json\n\u251c\u2500\u2500 scheduler\n\u2502   \u2514\u2500\u2500 scheduler_config.json\n\u251c\u2500\u2500 text_encoder\n\u2502   \u2514\u2500\u2500 model.onnx\n\u251c\u2500\u2500 tokenizer\n\u2502   \u251c\u2500\u2500 merges.txt\n\u2502   \u251c\u2500\u2500 special_tokens_map.json\n\u2502   \u251c\u2500\u2500 tokenizer_config.json\n\u2502   \u2514\u2500\u2500 vocab.json\n\u251c\u2500\u2500 unet\n\u2502   \u251c\u2500\u2500 model.onnx\n\u2502   \u2514\u2500\u2500 weights.pb\n\u251c\u2500\u2500 vae_decoder\n\u2502   \u2514\u2500\u2500 model.onnx\n\u2514\u2500\u2500 vae_encoder\n    \u2514\u2500\u2500 model.onnx\n")),(0,A.kt)("p",null,"There\u2019s a separate ONNX model for each Stable Diffusion subcomponent model."),(0,A.kt)("p",null,"Now to benchmark this similarly we can do the following:"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-python"},'from diffusers import OnnxStableDiffusionPipeline\npipe = OnnxStableDiffusionPipeline.from_pretrained("sd_onnx", provider="CUDAExecutionProvider")\n%time img = pipe("Iron man laughing", num_inference_steps=20, num_images_per_prompt=1).images[0]\nimg.save("iron_man.png", format="PNG")\n')),(0,A.kt)("p",null,"The overall performance results look great, at ~59% faster! We also didn\u2019t see any noticeable quality difference between the models."),(0,A.kt)("ul",null,(0,A.kt)("li",{parentName:"ul"},"Prompt: ",(0,A.kt)("em",{parentName:"li"},"Iron man laughing"),(0,A.kt)("ul",{parentName:"li"},(0,A.kt)("li",{parentName:"ul"},"ONNX inference latency: ",(0,A.kt)("inlineCode",{parentName:"li"},"1.34s")),(0,A.kt)("li",{parentName:"ul"},"HuggingFace version latency: ",(0,A.kt)("inlineCode",{parentName:"li"},"3.25s"))))),(0,A.kt)("p",null,"Since we know that the ",(0,A.kt)("inlineCode",{parentName:"p"},"unet")," model is the bottleneck, taking ~90% of the compute time, we can focus on it for further optimization. We try to serialize the ONNX version of the UNet to a TensorRT engine compatible format. When building the engine, the builder object selects the most optimized kernels for the chosen platform and configuration. Building the engine from a network definition file can be time consuming, and should not\xa0be repeated each time we need to perform inference\xa0unless the model/platform/configuration changes. You can transform the format of the engine after generation and save it to disk for later reuse (known as ",(0,A.kt)("a",{parentName:"p",href:"https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#serial_model_c"},(0,A.kt)("em",{parentName:"a"},"serializing")," the engine"),"). Deserializing occurs when you load the engine from disk into memory:"),(0,A.kt)("p",null,(0,A.kt)("a",{parentName:"p",href:"https://developer.nvidia.com/blog/speed-up-inference-tensorrt"},(0,A.kt)("img",{alt:"https://developer.nvidia.com/blog/speed-up-inference-tensorrt/",src:t(9526).Z,width:"625",height:"292"}))),(0,A.kt)("p",null,"To setup TensorRT properly, follow ",(0,A.kt)("a",{parentName:"p",href:"https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#requirements"},"this support table"),". It\u2019s a bit painful, and (similar to ",(0,A.kt)("a",{parentName:"p",href:"https://nvcr.io/cuda/cudnn"},"cuda/cudnn"),") if you just want a quick solution you can use NVIDIA\u2019s ",(0,A.kt)("a",{parentName:"p",href:"https://nvcr.io/nvidia/tensorrt:22.12-py3"},(0,A.kt)("inlineCode",{parentName:"a"},"tensorrt:22.12-py3")," docker image")," as a base:"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-docker"},"FROM nvcr.io/nvidia/tensorrt:22.12-py3\nENV CUDA_MODULE_LOADING=LAZY\nRUN pip install ipython transformers optimum[onnxruntime-gpu] onnx diffusers accelerate scipy safetensors composer\nRUN pip uninstall torch -y && pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121\nCOPY sd_onnx sd_onnx\n")),(0,A.kt)("p",null,"We can then use the following script for serialization:"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-python"},'import tensorrt as trt\nimport torch\n\nonnx_model = "sd_onnx/unet/model.onnx"\nengine_filename = "unet.trt" # saved serialized tensorrt engine file path\n# constants\nbatch_size = 1\nheight = 512\nwidth = 512\nlatents_shape = (batch_size, 4, height // 8, width // 8)\n# shape required by Stable Diffusion 2.1\'s UNet model\nembed_shape = (batch_size, 64, 1024)\ntimestep_shape = (batch_size,)\n\nTRT_LOGGER = trt.Logger(trt.Logger.INFO)\nTRT_BUILDER = trt.Builder(TRT_LOGGER)\nnetwork = TRT_BUILDER.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\nconfig = TRT_BUILDER.create_builder_config()\nprofile = TRT_BUILDER.create_optimization_profile()\n\nprint("Loading & validating ONNX model")\nonnx_parser = trt.OnnxParser(network, TRT_LOGGER)\nparse_success = onnx_parser.parse_from_file(onnx_model)\nfor idx in range(onnx_parser.num_errors):\n    print(onnx_parser.get_error(idx))\nif not parse_success:\n    raise ValueError("ONNX model parsing failed")\n\n# set input, latent and other shapes required by the layers\nprofile.set_shape("sample", latents_shape, latents_shape, latents_shape)\nprofile.set_shape("encoder_hidden_states", embed_shape, embed_shape, embed_shape)\nprofile.set_shape("timestep", timestep_shape, timestep_shape, timestep_shape)\nconfig.add_optimization_profile(profile)\n\nconfig.set_flag(trt.BuilderFlag.FP16)\nprint(f"Serializing & saving engine to \'{engine_filename}\'")\nserialized_engine = TRT_BUILDER.build_serialized_network(network, config)\nwith open(engine_filename, \'wb\') as f:\n    f.write(serialized_engine)\n')),(0,A.kt)("p",null,"Now let\u2019s move to deserializing ",(0,A.kt)("inlineCode",{parentName:"p"},"unet.trt")," for inference. We'll use the ",(0,A.kt)("inlineCode",{parentName:"p"},"TRTModel")," class from ",(0,A.kt)("a",{parentName:"p",href:"https://github.com/stochasticai/x-stable-diffusion/blob/main/TensorRT/trt_model.py"},"x-stable-diffusion's ",(0,A.kt)("inlineCode",{parentName:"a"},"trt_model")),":"),(0,A.kt)("pre",null,(0,A.kt)("code",{parentName:"pre",className:"language-python"},'import torch\nimport tensorrt as trt\ntrt.init_libnvinfer_plugins(None, "")\nimport pycuda.autoinit\nfrom diffusers import AutoencoderKL, LMSDiscreteScheduler\nfrom PIL import Image\nfrom torch import autocast\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom trt_model import TRTModel\nfrom tqdm.contrib import tenumerate\n\nclass TrtDiffusionModel:\n    def __init__(self):\n        self.device = torch.device("cuda")\n        self.unet = TRTModel("./unet.trt") # tensorrt engine saved path\n        self.vae = AutoencoderKL.from_pretrained(\n            "stabilityai/stable-diffusion-2-1", subfolder="vae").to(self.device)\n        self.tokenizer = CLIPTokenizer.from_pretrained(\n            "stabilityai/stable-diffusion-2-1", subfolder="tokenizer")\n        self.text_encoder = CLIPTextModel.from_pretrained(\n            "stabilityai/stable-diffusion-2-1", subfolder="text_encoder").to(self.device)\n        self.scheduler = LMSDiscreteScheduler(\n            beta_start=0.00085,\n            beta_end=0.012,\n            beta_schedule="scaled_linear",\n            num_train_timesteps=1000)\n\n    def predict(\n        self, prompts, num_inference_steps=50, height=512, width=512, max_seq_length=64\n    ):\n        guidance_scale = 7.5\n        batch_size = 1\n        text_input = self.tokenizer(\n            prompts,\n            padding="max_length",\n            max_length=max_seq_length,\n            truncation=True,\n            return_tensors="pt")\n        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n        uncond_input = self.tokenizer(\n            [""] * batch_size,\n            padding="max_length",\n            max_length=max_seq_length,\n            return_tensors="pt")\n        uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n        text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n        latents = torch.randn((batch_size, 4, height // 8, width // 8)).to(self.device)\n        self.scheduler.set_timesteps(num_inference_steps)\n        latents = latents * self.scheduler.sigmas[0]\n\n        with torch.inference_mode(), autocast("cuda"):\n            for i, t in tenumerate(self.scheduler.timesteps):\n                latent_model_input = torch.cat([latents] * 2)\n                sigma = self.scheduler.sigmas[i]\n                latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n                # predict the noise residual\n                inputs = [\n                    latent_model_input,\n                    torch.tensor([t]).to(self.device),\n                    text_embeddings]\n                noise_pred = self.unet(inputs, timing=True)\n                noise_pred = torch.reshape(noise_pred[0], (batch_size*2, 4, 64, 64))\n                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + guidance_scale * (\n                    noise_pred_text - noise_pred_uncond)\n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(noise_pred.cuda(), t, latents)["prev_sample"]\n            # scale and decode the image latents with VAE\n            latents = 1 / 0.18215 * latents\n            image = self.vae.decode(latents).sample\n        return image\n\nmodel = TrtDiffusionModel()\nimage = model.predict(\n    prompts="Iron man laughing, real photoshoot",\n    num_inference_steps=25,\n    height=512,\n    width=512,\n    max_seq_length=64)\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image.detach().cpu().permute(0, 2, 3, 1).numpy()\nimages = (image * 255).round().astype("uint8")\npil_images = [Image.fromarray(image) for image in images]\npil_images[0].save("image_generated.png")\n')),(0,A.kt)("p",null,"The above script runs, but the generated output looks like this:"),(0,A.kt)("table",null,(0,A.kt)("thead",{parentName:"table"},(0,A.kt)("tr",{parentName:"thead"},(0,A.kt)("th",{parentName:"tr",align:"center"},(0,A.kt)("img",{alt:"blank",src:t(8721).Z,width:"557",height:"561"})),(0,A.kt)("th",{parentName:"tr",align:"center"},(0,A.kt)("img",{alt:"noise",src:t(6065).Z,width:"379",height:"377"}))))),(0,A.kt)("p",null,"Something\u2019s going wrong, and changing to different tensor shapes (defined above) also doesn\u2019t help fix the generation of blank/noisy images."),(0,A.kt)("p",null,"I don't know how to make Stable Diffusion 2.1 work with TensorRT, though it's proved possible for other Stable Diffusion variants in ",(0,A.kt)("a",{parentName:"p",href:"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},"AUTOMATIC1111/stable-diffusion-webui"),". Others reporting similar issues in ",(0,A.kt)("a",{parentName:"p",href:"https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/5503#issuecomment-1341495770"},"stable-diffusion-webui#5503")," have suggested:"),(0,A.kt)("ul",null,(0,A.kt)("li",{parentName:"ul"},"Use more than 16-bits: I did, but it didn't help."),(0,A.kt)("li",{parentName:"ul"},"Use ",(0,A.kt)("inlineCode",{parentName:"li"},"xformers"),": For our model we need ",(0,A.kt)("a",{parentName:"li",href:"https://github.com/pytorch/pytorch/issues/97262"},(0,A.kt)("inlineCode",{parentName:"a"},"pytorch"),"'s recently added ",(0,A.kt)("inlineCode",{parentName:"a"},"scaled_dot_product_attention")," operator"),".")),(0,A.kt)("h2",{id:"other-frustrations"},"Other Frustrations"),(0,A.kt)("p",null,"Maybe the code above is paritally in my control, but there are also other issues that have nothing to do with my code:"),(0,A.kt)("ul",null,(0,A.kt)("li",{parentName:"ul"},"Licences: ",(0,A.kt)("a",{parentName:"li",href:"https://huggingface.github.io/text-generation-inference"},"Text Generation Inference")," recently they came up with ",(0,A.kt)("a",{parentName:"li",href:"https://twitter.com/jeffboudier/status/1685001126780026880?s=20"},"a new license")," which is more restrictive for newer versions. I can only use old releases (up to v0.9)."),(0,A.kt)("li",{parentName:"ul"},"Lack of GPU support: ",(0,A.kt)("a",{parentName:"li",href:"https://github.com/ggerganov/ggml"},"GGML")," doesn't currently support GPU inference, so I can't use it if I want very low latency."),(0,A.kt)("li",{parentName:"ul"},"Quality: I've heard from peers that saw a big decrease in output quality ",(0,A.kt)("a",{parentName:"li",href:"https://github.com/vllm-project/vllm"},"vLLM"),". I'd like to explore this in future.")),(0,A.kt)("h2",{id:"conclusion"},"Conclusion"),(0,A.kt)("p",null,"I've listed my recent errors and frustrations. I need more time to dig deeper and solve them, but if you think you can help please do reply in any of the issues linked above! By sharing my experiences and challenges, I hope this can spark lots of discussions and new ideas. Maybe you've faced something similar?"),(0,A.kt)("p",null,"While the world likes showcasing the latest advancements and shiny results, it's important to also acknowledge and address the underlying complexities that come with deploying & maintaining ML models. There's a scarcity of documentation/resources for these problems in the ML community. As the field continues to rapidly evolve, there is a need for more in-depth discussions and solutions to these technical hurdles."))}m.isMDXComponent=!0},6037:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/banner-b5aa93cec7efb70e4739aa558e1f04cc.png"},8721:(e,n,t)=>{t.d(n,{Z:()=>o});const o="data:image/png;base64,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"},6065:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/noise_image-bb7a3a7f753f1126b474a6952fa09b13.png"},9526:(e,n,t)=>{t.d(n,{Z:()=>o});const o=t.p+"assets/images/tensorrt-7a9261ec965b947b82c66f8810121e94.png"}}]);