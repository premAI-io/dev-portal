"use strict";(self.webpackChunkprem_docs=self.webpackChunkprem_docs||[]).push([[247],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>h});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),c=p(n),m=o,h=c["".concat(s,".").concat(m)]||c[m]||u[m]||r;return n?a.createElement(h,i(i({ref:t},d),{},{components:n})):a.createElement(h,i({ref:t},d))}));function h(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[c]="string"==typeof e?e:o,i[1]=l;for(var p=2;p<r;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},1989:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>p});var a=n(7462),o=(n(7294),n(3905));const r={slug:"serving-falcon-7b-fastapi-docker",title:"Serving Falcon 7B Instruct with FastAPI and Docker",authors:["filippopedrazzinfp"],tags:["llm","self-hosted","prem","open-source","fastapi","docker","falcon-7b"],description:"In this tutorial, we will walk you through the process of serving the Falcon 7B Instruction model using FastAPI and Docker. The complete code for this tutorial is available on GitHub.",image:"./banner.jpg"},i=void 0,l={permalink:"/blog/serving-falcon-7b-fastapi-docker",editUrl:"https://github.com/premAI-io/dev-portal/blob/main/blog/2023-07-03-serve-falcon-with-fastapi-and-docker/index.md",source:"@site/blog/2023-07-03-serve-falcon-with-fastapi-and-docker/index.md",title:"Serving Falcon 7B Instruct with FastAPI and Docker",description:"In this tutorial, we will walk you through the process of serving the Falcon 7B Instruction model using FastAPI and Docker. The complete code for this tutorial is available on GitHub.",date:"2023-07-03T00:00:00.000Z",formattedDate:"July 3, 2023",tags:[{label:"llm",permalink:"/blog/tags/llm"},{label:"self-hosted",permalink:"/blog/tags/self-hosted"},{label:"prem",permalink:"/blog/tags/prem"},{label:"open-source",permalink:"/blog/tags/open-source"},{label:"fastapi",permalink:"/blog/tags/fastapi"},{label:"docker",permalink:"/blog/tags/docker"},{label:"falcon-7b",permalink:"/blog/tags/falcon-7-b"}],readingTime:8.21,hasTruncateMarker:!0,authors:[{name:"Filippo Pedrazzini",title:"Core contributor @ PremAI",url:"https://github.com/filopedraz",imageURL:"https://github.com/filopedraz.png",key:"filippopedrazzinfp"}],frontMatter:{slug:"serving-falcon-7b-fastapi-docker",title:"Serving Falcon 7B Instruct with FastAPI and Docker",authors:["filippopedrazzinfp"],tags:["llm","self-hosted","prem","open-source","fastapi","docker","falcon-7b"],description:"In this tutorial, we will walk you through the process of serving the Falcon 7B Instruction model using FastAPI and Docker. The complete code for this tutorial is available on GitHub.",image:"./banner.jpg"},prevItem:{title:"Talk to your Data with ChainLit and Langchain",permalink:"/blog/chainlit-langchain-prem"},nextItem:{title:"Build a Perplexity AI clone on Prem",permalink:"/blog/perplexity-ai-self-hosted"}},s={image:n(7980).Z,authorsImageUrls:[void 0]},p=[{value:"If you want to just use the model for inference directly, you can use our pre-built docker image like this:",id:"if-you-want-to-just-use-the-model-for-inference-directly-you-can-use-our-pre-built-docker-image-like-this",level:5},{value:"Step 1: Setup the Python Server",id:"step-1-setup-the-python-server",level:3},{value:"1. Create <code>requirements.txt</code> file.",id:"1-create-requirementstxt-file",level:4},{value:"Step 2: Expose Falcon 7B Instruct with FastAPI",id:"step-2-expose-falcon-7b-instruct-with-fastapi",level:3},{value:"1. Create <code>models.py</code> file.",id:"1-create-modelspy-file",level:4},{value:"2. Create <code>utils.py</code> file.",id:"2-create-utilspy-file",level:4},{value:"3. Create <code>routes.py</code> file.",id:"3-create-routespy-file",level:4},{value:"4. Create <code>main.py</code> file.",id:"4-create-mainpy-file",level:4},{value:"Step 3: Use Docker to build and run the application",id:"step-3-use-docker-to-build-and-run-the-application",level:3},{value:"1. Create a <code>download.py</code> file.",id:"1-create-a-downloadpy-file",level:4},{value:"2. Create a <code>Dockerfile</code>.",id:"2-create-a-dockerfile",level:4},{value:"4. Create a <code>.dockerignore</code> file.",id:"4-create-a-dockerignore-file",level:4},{value:"Step 4: Build and run the application",id:"step-4-build-and-run-the-application",level:3},{value:"1. Build the Docker image.",id:"1-build-the-docker-image",level:4},{value:"2. Run the Docker image.",id:"2-run-the-docker-image",level:4},{value:"Conclusion",id:"conclusion",level:3}],d={toc:p},c="wrapper";function u(e){let{components:t,...r}=e;return(0,o.kt)(c,(0,a.Z)({},d,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Prem Banner",src:n(7980).Z,width:"2400",height:"1350"})),(0,o.kt)("head",null,(0,o.kt)("meta",{name:"twitter:image",content:"./banner.jpg"})),(0,o.kt)("p",null,"In this tutorial, we will walk you through the process of serving the Falcon 7B Instruction model using FastAPI and Docker. The complete code for this tutorial is available on ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/premAI-io/llm-fastapi-docker-template"},"GitHub"),"."),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"NOTE: in order to run Falcon 7B Instruct model you will need a GPU with at least 16GiB of VRAM. You can use a ",(0,o.kt)("a",{parentName:"p",href:"https://www.paperspace.com/gpu-cloud"},"Paperspace Cloud")," virtual server or any other cloud provider or your own server with a NVIDIA GPU.")),(0,o.kt)("h5",{id:"if-you-want-to-just-use-the-model-for-inference-directly-you-can-use-our-pre-built-docker-image-like-this"},"If you want to just use the model for inference directly, you can use our pre-built docker image like this:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"docker run --gpus all -p 8000:8000 ghcr.io/premai-io/chat-falcon-7b-instruct-gpu:latest\n")),(0,o.kt)("p",null,"This will ensure that the container has access to the GPU and will expose the API on port 8000. ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/premAI-io/prem-registry/blob/dev/chat-falcon-7b-instruct/README.md"},"Learn more"),"."),(0,o.kt)("h3",{id:"step-1-setup-the-python-server"},"Step 1: Setup the Python Server"),(0,o.kt)("p",null,"First, we need to create a requirements.txt file to list all the necessary dependencies. This file will include libraries such as FastAPI, uvicorn, pytest, requests, tqdm, httpx, python-dotenv, tenacity, einops, sentencepiece, accelerate, and xformers."),(0,o.kt)("h4",{id:"1-create-requirementstxt-file"},"1. Create ",(0,o.kt)("inlineCode",{parentName:"h4"},"requirements.txt")," file."),(0,o.kt)("p",null,"Create a ",(0,o.kt)("inlineCode",{parentName:"p"},"requirements.txt")," file with the following dependencies:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-txt"},"fastapi==0.95.0\nuvicorn==0.21.1\npytest==7.2.2\nrequests==2.28.2\ntqdm==4.65.0\nhttpx==0.23.3\npython-dotenv==1.0.0\ntenacity==8.2.2\neinops==0.6.1\nsentencepiece==0.1.99\naccelerate>=0.16.0,<1\nxformers==0.0.20\n")),(0,o.kt)("h3",{id:"step-2-expose-falcon-7b-instruct-with-fastapi"},"Step 2: Expose Falcon 7B Instruct with FastAPI"),(0,o.kt)("p",null,"Next, we will create a ",(0,o.kt)("inlineCode",{parentName:"p"},"models.py")," file to define the model class that will be used to serve the Falcon 7B Instruction model. We will use the ",(0,o.kt)("inlineCode",{parentName:"p"},"transformers")," library to fetch the model from the HuggingFace Hub."),(0,o.kt)("p",null,"We will also need a ",(0,o.kt)("inlineCode",{parentName:"p"},"utils.py")," file to define the stopping criteria for the Falcon model. This criteria is used to signal the model when to stop generating new tokens."),(0,o.kt)("p",null,"Finally, we will create a ",(0,o.kt)("inlineCode",{parentName:"p"},"routes.py")," file to define the endpoints that our FastAPI web server will handle. This file will include the logic for generating responses and handling exceptions."),(0,o.kt)("h4",{id:"1-create-modelspy-file"},"1. Create ",(0,o.kt)("inlineCode",{parentName:"h4"},"models.py")," file."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import os\nimport torch\n\nfrom typing import List\nfrom transformers import AutoTokenizer, Pipeline, pipeline\n\nclass FalconBasedModel(object):\n    model = None\n    stopping_criteria = None\n\n    @classmethod\n    def generate(\n        cls,\n        messages: list,\n        temperature: float = 0.9,\n        top_p: float = 0.9,\n        n: int = 1,\n        stream: bool = False,\n        max_tokens: int = 256,\n        stop: str = "",\n        **kwargs,\n    ) -> List:\n        message = messages[-1]["content"]\n        return [\n            cls.model(\n                message,\n                max_length=max_tokens,\n                num_return_sequences=n,\n                temperature=temperature,\n                top_p=top_p,\n                eos_token_id=cls.tokenizer.eos_token_id,\n                return_full_text=kwargs.get("return_full_text", False),\n                do_sample=kwargs.get("do_sample", True),\n                stop_sequence=stop[0] if stop else None,\n                stopping_criteria=cls.stopping_criteria(stop, message, cls.tokenizer),\n            )[0]["generated_text"]\n        ]\n\n    @classmethod\n    def get_model(cls) -> Pipeline:\n        if cls.model is None:\n            cls.tokenizer = AutoTokenizer.from_pretrained(\n                os.getenv("MODEL_ID", "tiiuae/falcon-7b-instruct"),\n                trust_remote_code=True,\n            )\n            cls.model = pipeline(\n                tokenizer=cls.tokenizer,\n                model=os.getenv("MODEL_ID", "tiiuae/falcon-7b-instruct"),\n                torch_dtype=torch.bfloat16,\n                trust_remote_code=True,\n                device_map=os.getenv("DEVICE", "auto"),\n            )\n        cls.stopping_criteria = FalconStoppingCriteria\n        return cls.model\n\n')),(0,o.kt)("p",null,"In the provided code, a class named ",(0,o.kt)("inlineCode",{parentName:"p"},"FalconBasedModel")," is used to encapsulate the functionality related to the Falcon 7B Instruction model. This class-based approach has several advantages:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Encapsulation"),": By using a class, we can bundle together the model, its tokenizer, and its stopping criteria into a single unit. This makes the code more organized and easier to understand. It also allows us to hide the internal details of how the model works, exposing only the methods that are necessary for interacting with it.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"State Preservation"),": Class methods can access and modify the state of an instance of the class. In this case, the ",(0,o.kt)("inlineCode",{parentName:"p"},"FalconBasedModel")," class maintains the state of the model and its tokenizer. This is useful because it allows us to load the model and tokenizer only once, when the ",(0,o.kt)("inlineCode",{parentName:"p"},"get_model")," method is first called, and then reuse them for subsequent calls to the ",(0,o.kt)("inlineCode",{parentName:"p"},"generate")," method. This can significantly improve performance, as loading a model and tokenizer can be computationally expensive operations."))),(0,o.kt)("h4",{id:"2-create-utilspy-file"},"2. Create ",(0,o.kt)("inlineCode",{parentName:"h4"},"utils.py")," file."),(0,o.kt)("p",null,"Like other generative AI models, Falcon requires a stopping criteria to determine when to cease generating new tokens. We will use a straightforward stopping criteria that checks if the target sequence is present in the generated text. The default value is set to 'User:', but developers can provide a custom target sequence through APIs."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'\nfrom typing import List\n\nfrom transformers import StoppingCriteria\n\nclass FalconStoppingCriteria(StoppingCriteria):\n    def __init__(self, target_sequences: List[str], prompt, tokenizer) -> None:\n        self.target_sequences = target_sequences\n        self.prompt = prompt\n        self.tokenizer = tokenizer\n\n    def __call__(self, input_ids, scores, **kwargs) -> bool:\n        if not self.target_sequences:\n            return False\n        # Get the generated text as a string\n        generated_text = self.tokenizer.decode(input_ids[0])\n        generated_text = generated_text.replace(self.prompt, "")\n        # Check if the target sequence appears in the generated text\n        return any(\n            target_sequence in generated_text\n            for target_sequence in self.target_sequences\n        )\n\n    def __len__(self) -> int:\n        return len(self.target_sequences)\n\n    def __iter__(self):\n        yield self\n')),(0,o.kt)("h4",{id:"3-create-routespy-file"},"3. Create ",(0,o.kt)("inlineCode",{parentName:"h4"},"routes.py")," file."),(0,o.kt)("p",null,"Next, we'll define the endpoints that our FastAPI web server will handle."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import json\nimport os\nimport uuid\nfrom datetime import datetime as dt\nfrom typing import Any, Dict, Generator, List, Optional, Union\n\nfrom fastapi import APIRouter, HTTPException\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\n\nfrom models import FalconBasedModel as model\n\n\nclass ChatCompletionInput(BaseModel):\n    model: str\n    messages: List[dict]\n    temperature: float = 1.0\n    top_p: float = 1.0\n    n: int = 1\n    stream: bool = False\n    stop: Optional[Union[str, List[str]]] = ["User:"]\n    max_tokens: int = 64\n    presence_penalty: float = 0.0\n    frequence_penalty: float = 0.0\n    logit_bias: Optional[dict] = {}\n    user: str = ""\n\n\nclass ChatCompletionResponse(BaseModel):\n    id: str = uuid.uuid4()\n    model: str\n    object: str = "chat.completion"\n    created: int = int(dt.now().timestamp())\n    choices: List[dict]\n    usage: dict = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}\n\n\nrouter = APIRouter()\n\n\nasync def generate_chunk_based_response(body, text) -> Generator[str, Any, None]:\n    yield "event: completion\\ndata: " + json.dumps(\n        {\n            "id": str(uuid.uuid4()),\n            "model": body.model,\n            "object": "chat.completion",\n            "choices": [\n                {\n                    "role": "assistant",\n                    "index": 1,\n                    "delta": {"role": "assistant", "content": text},\n                    "finish_reason": "stop",\n                }\n            ],\n            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},\n        }\n    ) + "\\n\\n"\n    yield "event: done\\ndata: [DONE]\\n\\n"\n\n\n@router.post("/chat/completions", response_model=ChatCompletionResponse)\nasync def chat_completions(body: ChatCompletionInput) -> Dict[str, Any]:\n    try:\n        predictions = model.generate(\n            messages=body.messages,\n            temperature=body.temperature,\n            top_p=body.top_p,\n            n=body.n,\n            stream=body.stream,\n            max_tokens=body.max_tokens,\n            stop=body.stop,\n        )\n        if body.stream:\n            return StreamingResponse(\n                generate_chunk_based_response(body, predictions[0]),\n                media_type="text/event-stream",\n            )\n        return ChatCompletionResponse(\n            id=str(uuid.uuid4()),\n            model=os.getenv("MODEL_ID", "tiiuae/falcon-7b-instruct"),\n            object="chat.completion",\n            created=int(dt.now().timestamp()),\n            choices=[\n                {\n                    "role": "assistant",\n                    "index": idx,\n                    "message": {"role": "assistant", "content": text},\n                    "finish_reason": "stop",\n                }\n                for idx, text in enumerate(predictions)\n            ],\n            usage={"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},\n        )\n    except ValueError as error:\n        raise HTTPException(\n            status_code=400,\n            detail={"message": str(error)},\n        )\n\n')),(0,o.kt)("h4",{id:"4-create-mainpy-file"},"4. Create ",(0,o.kt)("inlineCode",{parentName:"h4"},"main.py")," file."),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"main.py")," file is the entry point for our FastAPI application. It is responsible for setting up the application and starting the server."),(0,o.kt)("p",null,"One important aspect of this file is the ",(0,o.kt)("inlineCode",{parentName:"p"},"create_start_app_handler")," function. This function is designed to be called when the FastAPI application starts up. It creates a function, ",(0,o.kt)("inlineCode",{parentName:"p"},"start_app"),", that is responsible for loading the Falcon 7B Instruction model into memory. This is done by calling the ",(0,o.kt)("inlineCode",{parentName:"p"},"get_model")," method of the ",(0,o.kt)("inlineCode",{parentName:"p"},"FalconBasedModel")," class."),(0,o.kt)("p",null,"The reason we load the model into memory at startup is to improve the performance of our application. Loading a model is a time-consuming operation. If we were to load the model every time we needed to use it, it would significantly slow down our application. By loading the model at startup, we ensure that it's done only once, no matter how many requests our application needs to handle."),(0,o.kt)("p",null,"The ",(0,o.kt)("inlineCode",{parentName:"p"},"start_app")," function is then returned and registered as a startup event handler for our FastAPI application. This means that FastAPI will automatically call this function when the application starts up, ensuring that our model is loaded and ready to use."),(0,o.kt)("p",null,"The rest of the ",(0,o.kt)("inlineCode",{parentName:"p"},"main.py")," file is responsible for setting up the FastAPI application, including registering our API routes and setting up CORS (Cross-Origin Resource Sharing) middleware. Finally, if this file is run directly (i.e., it is the main module), it starts the FastAPI server using uvicorn."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import logging\nfrom typing import Callable\n\nimport uvicorn\nfrom dotenv import load_dotenv\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom routes import router as api_router\n\nfrom models import FalconBasedModel\n\ndef create_start_app_handler(app: FastAPI) -> Callable[[], None]:\n    def start_app() -> None:\n        FalconBasedModel.get_model()\n\n    return start_app\n\n\ndef get_application() -> FastAPI:\n    application = FastAPI(title="prem-chat-falcon", debug=True, version="0.0.1")\n    application.include_router(api_router, prefix="/v1")\n    application.add_event_handler("startup", create_start_app_handler(application))\n    application.add_middleware(\n        CORSMiddleware,\n        allow_origins=["*"],\n        allow_credentials=True,\n        allow_methods=["*"],\n        allow_headers=["*"],\n    )\n    return application\n\n\napp = get_application()\n\n\nif __name__ == "__main__":\n    uvicorn.run("main:app", host="0.0.0.0", port=8000)\n\n')),(0,o.kt)("h3",{id:"step-3-use-docker-to-build-and-run-the-application"},"Step 3: Use Docker to build and run the application"),(0,o.kt)("p",null,"To build and run the application, we will first create a ",(0,o.kt)("inlineCode",{parentName:"p"},"download.py")," file. This script will be called at build time to download the model and cache it in the Docker image."),(0,o.kt)("p",null,"Next, we will create a Dockerfile that uses the official image from HuggingFace, which includes all the necessary dependencies. This Dockerfile will define the steps to build our Docker image."),(0,o.kt)("p",null,"To avoid including any unused files in the build process, we will also create a ",(0,o.kt)("inlineCode",{parentName:"p"},".dockerignore")," file."),(0,o.kt)("h4",{id:"1-create-a-downloadpy-file"},"1. Create a ",(0,o.kt)("inlineCode",{parentName:"h4"},"download.py")," file."),(0,o.kt)("p",null,"The download script will be called at build time to download the model and cache it in the Docker image."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import argparse\nimport os\n\nimport torch\nimport transformers\nfrom tenacity import retry, stop_after_attempt, wait_fixed\nfrom transformers import AutoTokenizer\n\nparser = argparse.ArgumentParser()\nparser.add_argument("--model", help="Model to download")\nargs = parser.parse_args()\n\nprint(f"Downloading model {args.model}")\n\n\n@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))\ndef download_model() -> None:\n    _ = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)\n    _ = transformers.pipeline(\n        model=args.model,\n        torch_dtype=torch.bfloat16,\n        trust_remote_code=True,\n        device_map=os.getenv("DEVICE", "auto"),\n    )\n\n\ndownload_model()\n\n')),(0,o.kt)("h4",{id:"2-create-a-dockerfile"},"2. Create a ",(0,o.kt)("inlineCode",{parentName:"h4"},"Dockerfile"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-dockerfile"},"FROM huggingface/transformers-pytorch-gpu:4.28.1\n\nARG MODEL_ID\n\nWORKDIR /usr/src/app/\n\nCOPY requirements.txt ./\n\nRUN pip install --no-cache-dir -r ./requirements.txt --upgrade pip\n\nCOPY download.py .\n\nRUN python3 download.py --model $MODEL_ID\n\nCOPY . .\n\nENV MODEL_ID=$MODEL_ID\n\nCMD python3 main.py\n")),(0,o.kt)("h4",{id:"4-create-a-dockerignore-file"},"4. Create a ",(0,o.kt)("inlineCode",{parentName:"h4"},".dockerignore")," file."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-dockerfile"},".editorconfig\n.gitattributes\n.github\n.gitignore\n.gitlab-ci.yml\n.idea\n.pre-commit-config.yaml\n.readthedocs.yml\n.travis.yml\nvenv\n.git\n./ml/models/\n.bin\n")),(0,o.kt)("h3",{id:"step-4-build-and-run-the-application"},"Step 4: Build and run the application"),(0,o.kt)("p",null,"Finally, we will build the Docker image using the ",(0,o.kt)("inlineCode",{parentName:"p"},"docker build")," command and run it using the ",(0,o.kt)("inlineCode",{parentName:"p"},"docker run")," command."),(0,o.kt)("h4",{id:"1-build-the-docker-image"},"1. Build the Docker image."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'docker build --file ./Dockerfile \\\n    --build-arg="MODEL_ID=tiiuae/falcon-7b-instruct" \\\n    --tag blog-post/chat-falcon-7b-instruct-gpu:latest \\\n    --tag blog-post/chat-falcon-7b-instruct-gpu:0.0.1 \\\n    .\n')),(0,o.kt)("h4",{id:"2-run-the-docker-image"},"2. Run the Docker image."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"docker run --gpus all -p 8000:8000 blog-post/chat-falcon-7b-instruct-gpu:latest\n")),(0,o.kt)("h3",{id:"conclusion"},"Conclusion"),(0,o.kt)("p",null,"In this tutorial, we have demonstrated how to serve the Falcon 7B Instruction model using FastAPI and Docker. This is a crucial first step in serving a model for production use cases."))}u.isMDXComponent=!0},7980:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/banner-e4e9bf8c22ab57b00f8669ed27e04210.jpg"}}]);