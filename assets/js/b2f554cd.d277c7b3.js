"use strict";(self.webpackChunkprem_docs=self.webpackChunkprem_docs||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"prem-app-network-mode","metadata":{"permalink":"/blog/prem-app-network-mode","editUrl":"https://github.com/premAI-io/dev-portal/blob/main/blog/2023-12-04-prem-app-network-mode/index.md","source":"@site/blog/2023-12-04-prem-app-network-mode/index.md","title":"Contribute GPU to the Prem Network","description":"Contribute GPU to the Prem Network and help us to build the largest computer in the world to run AI inference with consumer devices.","date":"2023-12-04T00:00:00.000Z","formattedDate":"December 4, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"ai","permalink":"/blog/tags/ai"},{"label":"self-hosted","permalink":"/blog/tags/self-hosted"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"on-premise","permalink":"/blog/tags/on-premise"},{"label":"open-source","permalink":"/blog/tags/open-source"},{"label":"peer-to-peer","permalink":"/blog/tags/peer-to-peer"},{"label":"gpu","permalink":"/blog/tags/gpu"},{"label":"prem-network","permalink":"/blog/tags/prem-network"}],"readingTime":1.375,"hasTruncateMarker":true,"authors":[{"name":"Marco Argentieri","title":"Bitcoin wizard","url":"https://github.com/tiero","imageURL":"https://github.com/tiero.png","key":"tiero"}],"frontMatter":{"slug":"prem-app-network-mode","title":"Contribute GPU to the Prem Network","authors":["tiero"],"tags":["llm","ai","self-hosted","prem","on-premise","open-source","peer-to-peer","gpu","prem-network"],"description":"Contribute GPU to the Prem Network and help us to build the largest computer in the world to run AI inference with consumer devices.","image":"./image.png"},"nextItem":{"title":"Prem Network Unveiled: Democratizing AI with Peer-to-Peer GPU Networking","permalink":"/blog/prem-network-launch-p2p-gpu"}},"content":"If you have installed Prem App on your Mac with Apple Silicon, you can contribute to the Prem Network by running a Prem node on your machine. In order to do that, you can follow the instructions below.\\n\\n\x3c!--truncate--\x3e\\n\\n## 1. Install Prem on your Mac\\n\\nYou must have Prem App installed on your MacOS with Apple Silicon [here](https://install-app.prem.ninja/latest-release) in order to provide inference capacity to the Prem Network.\\n\\n## 2. Join the Network\\n\\n1. Click on `Settings` in the to and scroll down to `Prem Network` section.\\n\\n![Network Init](./network-init.png)\\n\\n- Select the Model you wanna contribute to.\\n- Select the number of blocks you want to contribute to.\\n- Click on `Start` button.\\n\\nAs soon as you Click on the `Start` button, the Prem App will start downloading the blocks from the network and will start contributing to the network. The process requires a coouple of minutes to complete. In the meantime, you will see the following message:\\n\\n![Network Install](./network-install.png)\\n\\nWhen the process is completed, a pop-up will appear with the following message:\\n\\n![Network Done](./network-done.png)\\n\\n## 3. Check your contribution\\n\\nYou can now go to https://network.premai.io to check your contribution to the network.\\n\\n## 4. Stop contributing\\n\\nYou can always stop to contribute, clicking on the `Stop` button.\\n\\n![Network Stop](./network-stop.png)\\n\\n## 5. Delete the dependencies\\n\\nIf you want to delete the environment and start fresh, you can click on the `Delete` icon and the udnerlying dependencies will be deleted.\\n\\n![Network Delete](./network-delete.png)\\n\\n## 5. Make open-source AI a reality\\n\\nYou can now contribute to the Prem Network and help us to build the largest computer in the world to run AI inference with consumer devices."},{"id":"prem-network-launch-p2p-gpu","metadata":{"permalink":"/blog/prem-network-launch-p2p-gpu","editUrl":"https://github.com/premAI-io/dev-portal/blob/main/blog/2023-11-24-peer-to-peer-gpu-network/index.md","source":"@site/blog/2023-11-24-peer-to-peer-gpu-network/index.md","title":"Prem Network Unveiled: Democratizing AI with Peer-to-Peer GPU Networking","description":"Discover Prem Network, a cutting-edge peer-to-peer platform that revolutionizes Open Source AI processing. Learn how its peer-to-peer GPU networking enables efficient handling of large models, offering a scalable solution for AI enthusiasts, researchers and developers constrained by limited resources or censorship.","date":"2023-11-24T00:00:00.000Z","formattedDate":"November 24, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"ai","permalink":"/blog/tags/ai"},{"label":"self-hosted","permalink":"/blog/tags/self-hosted"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"on-premise","permalink":"/blog/tags/on-premise"},{"label":"open-source","permalink":"/blog/tags/open-source"},{"label":"peer-to-peer","permalink":"/blog/tags/peer-to-peer"},{"label":"gpu","permalink":"/blog/tags/gpu"},{"label":"prem-network","permalink":"/blog/tags/prem-network"}],"readingTime":3.085,"hasTruncateMarker":true,"authors":[{"name":"Marco Argentieri","title":"Bitcoin wizard","url":"https://github.com/tiero","imageURL":"https://github.com/tiero.png","key":"tiero"}],"frontMatter":{"slug":"prem-network-launch-p2p-gpu","title":"Prem Network Unveiled: Democratizing AI with Peer-to-Peer GPU Networking","authors":["tiero"],"tags":["llm","ai","self-hosted","prem","on-premise","open-source","peer-to-peer","gpu","prem-network"],"description":"Discover Prem Network, a cutting-edge peer-to-peer platform that revolutionizes Open Source AI processing. Learn how its peer-to-peer GPU networking enables efficient handling of large models, offering a scalable solution for AI enthusiasts, researchers and developers constrained by limited resources or censorship.","image":"./image.png"},"prevItem":{"title":"Contribute GPU to the Prem Network","permalink":"/blog/prem-app-network-mode"},"nextItem":{"title":"Introducing Prem App v0.2.x: Enhanced Speed and Simplified Usage Without Docker","permalink":"/blog/prem-app-v0-2-enhanced-speed-without-docker"}},"content":"\x3c!--truncate--\x3e\\n\\n<head>\\n  <meta name=\\"twitter:image\\" content=\\"./image.png\\"/>\\n</head>\\n\\nWe\'re thrilled to announce the launch of the [Prem Network](https://premai.io/prem-network), a peer-to-peer network poised to transform the landscape of open source AI. Building upon the foundations laid by our recently updated [Prem App](../2023-11-15-prem-app-v0.2-released/index.md), the Prem Network mode is seamlessly integrated into the app, enabling users to leverage the network\'s for inference and fine-tuning, split accross multiple devices.\\n\\n\x3c!--truncate--\x3e\\n\\n![Prem Network world globe](./image.png)\\n\\n## The Vision of Prem Network\\n\\nOur vision with the Prem Network is to break down the barriers in generative AI. It leverages a distributed mesh of GPU providers to distribute open-source large language models. This means individuals and smaller companies can now compete on the same playing field as **Big AI**, using technology that was previously out of reach.\\n\\n## How It Works\\n\\nThe network operates on a simple yet effective principle: process sensitive layers locally while offloading additional workloads to peers across the network. This peer-to-peer swarm networking approach enables users to increase the size of models they can run locally, effectively turning multiple devices into a singular, powerful computational resource.\\nIntegral to the Prem Network\'s prowess is its innovative tech stack, comprising BitTorrent, Petals, Nostr, and LibP2P. These cutting-edge technologies synergize to provide a robust and efficient backbone for our network.\\n\\n- [BitTorrent](https://www.bittorrent.com): The world\'s most popular peer-to-peer protocol, BitTorrent is the foundation of our network\'s peer-to-peer file-sharing. It enables the network to efficiently distribute model, weights and data repositories across the network of Prem clients.\\n\\n- [Nostr](https://nostr.com): A simple, open protocol that enables a truly censorship-resistant and global social network, used as overlay and discovery protocol for the Prem clients.\\n\\n- [Petals](https://petals.dev): You can then run large language models at home, BitTorrent\u2011style. The most sensitive model layers run locally, and the network will automatically fetch the other layers from other peers.\\n\\n- [LibP2P](https://libp2p.io): A foundational element, LibP2P underpins our network\'s communication infrastructure, ensuring secure and reliable peer-to-peer interactions within the network.\\n\\nTogether, these technologies empower the Prem Network to handle the complex demands of modern AI applications, making it a powerhouse for both individual enthusiasts and large-scale enterprises.\\n\\n## Public explorer\\n\\nIn tandem with the Prem Network\'s launch, we\'re excited to unveil [Petals Explorer](https://network.premai.io) - a complementary user interface that synergizes with the Prem Network. Petals Explorer offers a unique approach to running large language models (LLMs) by adopting a BitTorrent-style distribution mechanism. This integration marks a significant stride in our mission to democratize AI. It allows users to effortlessly tap into the power of LLMs like Llama 2, Falcon, and BLOOM, ensuring efficient operation even on consumer-grade GPUs. The collaboration of Petals with the Prem Network epitomizes our commitment to accessible, open-source, and peer-to-peer AI solutions, setting a new standard in the AI community.\\n\\n## Open to All\\n\\nEchoing the inclusive spirit of our [Prem App v0.2](../2023-11-15-prem-app-v0.2-released/index.md), the Prem Network is accessible to both individuals and businesses. Whether you\'re an AI enthusiast, a researcher, a startup, or an enterprise, the network is designed to cater to your computational needs, without the cost and complexity of traditional setups.\\n\\n## Supported Models and Security\\n\\nThe initial models available on the network, like Stable Beluga 2 and Falcon 180B, illustrate our commitment to providing a versatile range of options for various AI applications. Furthermore, the security of your data remains paramount, with sensitive data processed locally and only non-sensitive parts delegated to the network.\\n\\n## Join the Network\\n\\n Whether you\'re [contributing GPU using the Prem Desktop App](https://dev.premai.io/docs/prem-app/network-mode/) or simply leveraging the network\'s capabilities to make peer-to-peer inference requests, the Prem Network is a powerful tool for democratizing AI. We\'re excited to see how the network will evolve as more users join the network and contribute their GPUs."},{"id":"prem-app-v0-2-enhanced-speed-without-docker","metadata":{"permalink":"/blog/prem-app-v0-2-enhanced-speed-without-docker","editUrl":"https://github.com/premAI-io/dev-portal/blob/main/blog/2023-11-15-prem-app-v0.2-released/index.md","source":"@site/blog/2023-11-15-prem-app-v0.2-released/index.md","title":"Introducing Prem App v0.2.x: Enhanced Speed and Simplified Usage Without Docker","description":"Discover the new Prem App v0.2: Enhanced for Mac with Apple Silicon. Experience faster AI inference without Docker. Now featuring models like Mistral Instruct 7B, Whisper Tiny, MiniLM L6 v2, and more. Ideal for developers and AI enthusiasts seeking advanced AI capabilities on Mac OS. Upgrade your AI tools with Prem App\'s latest version","date":"2023-11-15T00:00:00.000Z","formattedDate":"November 15, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"ai","permalink":"/blog/tags/ai"},{"label":"self-hosted","permalink":"/blog/tags/self-hosted"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"on-premise","permalink":"/blog/tags/on-premise"},{"label":"open-source","permalink":"/blog/tags/open-source"},{"label":"mistral","permalink":"/blog/tags/mistral"},{"label":"whisper","permalink":"/blog/tags/whisper"},{"label":"tabby","permalink":"/blog/tags/tabby"}],"readingTime":1.895,"hasTruncateMarker":true,"authors":[{"name":"Marco Argentieri","title":"Bitcoin wizard","url":"https://github.com/tiero","imageURL":"https://github.com/tiero.png","key":"tiero"}],"frontMatter":{"slug":"prem-app-v0-2-enhanced-speed-without-docker","title":"Introducing Prem App v0.2.x: Enhanced Speed and Simplified Usage Without Docker","authors":["tiero"],"tags":["llm","ai","self-hosted","prem","on-premise","open-source","mistral","whisper","tabby"],"description":"Discover the new Prem App v0.2: Enhanced for Mac with Apple Silicon. Experience faster AI inference without Docker. Now featuring models like Mistral Instruct 7B, Whisper Tiny, MiniLM L6 v2, and more. Ideal for developers and AI enthusiasts seeking advanced AI capabilities on Mac OS. Upgrade your AI tools with Prem App\'s latest version","image":"./image.png"},"prevItem":{"title":"Prem Network Unveiled: Democratizing AI with Peer-to-Peer GPU Networking","permalink":"/blog/prem-network-launch-p2p-gpu"},"nextItem":{"title":"Install Prem on AWS","permalink":"/blog/install-prem-on-aws-llmops-in-production"}},"content":"\x3c!--truncate--\x3e\\n\\n<head>\\n  <meta name=\\"twitter:image\\" content=\\"./image.png\\"/>\\n</head>\\n\\nWe are excited to announce the release of Prem App v0.2.x. This update brings performance improvements and new features, enhancing your private and sovereign generative AI experience on Mac OS with Apple Silicon chip.\\n\\n![Prem Desktop App with model gallery](./image.png)\\n\\n\ud83d\udc49 **[Dowload the latest Prem Desktop App now for MacOS with Apple chip](https://install-app.prem.ninja/latest-release)**\\n\\nHere\'s what you can expect:\\n\\n### No More Docker Dependency\\nWe\'ve listened to your feedback and removed the need for Docker. This means a simpler, more streamlined setup process for all users.\\n\\n### Rust-Powered Binary Controller\\nOur new Rust binary controller efficiently manages the lifecycle of AI models running locally. It handles everything from downloading to running, stopping, and deleting AI model binaries from the user interface.\\n\\n### Optimized for Apple Silicon\\nPrem App v0.2 fully leverages the power of Apple Silicon GPUs thanks to [GGML](https://ggml.ai)-based models built on top of [llama.cpp](https://github.com/ggerganov/llama.cpp) and [whisper.cpp](https://github.com/ggerganov/whisper.cpp). \\nThis enables larger models and higher inference speed on commodity hardware.\\n\\n### A Diverse Range of AI Models and Vector Stores\\nWe\'re proud to offer a wide range of AI models and Vector Stores to cater to various needs and applications, from chat completions to full RAG apps. With Prem App v0.2, you have access to:\\n\\n- [**Mistral Instruct 7B**](https://registry.premai.io/detail.html?service=mistral-7b-instruct): A versatile model for a range of instructive tasks.\\n- [**Mistral 7B with 128k Context**](https://registry.premai.io/detail.html?service=mistral-7b-128k): Ideal for handling extensive context requirements.\\n- [**Whisper Tiny**](https://registry.premai.io/detail.html?service=whisper-tiny-cpp): A compact yet powerful model for speech-to-text needs.\\n- [**All MiniLM L6 v2**](https://registry.premai.io/detail.html?service=all-minilm-l6-v2): Perfect for generating embeddings with high accuracy.\\n- [**Tabby StarCoder 1B**](https://registry.premai.io/detail.html?service=tabby-starcoder-1b): Tailored for coding and programming-related tasks.\\n- [**Tabby CodeLLama 7B**](https://registry.premai.io/detail.html?service=tabby-codellama-7B): Another excellent option for developers and programmers.\\n- [**Qdrant**](https://registry.premai.io/detail.html?service=qdrant): A robust vector store database for your data management needs.\\n\\nThese models and stores represent the cutting edge in open-source generative AI, ensuring you have the tools you need for a wide range of applications.\\n\\n\\n### Conclusion\\nPrem App v0.2 is more than an update; it\'s a leap forward in local inference on Mac OS for Apple Silicon users. It\'s about giving you faster, more efficient, and high-quality AI capabilities without the complexity. We\'re excited for you to experience these advancements and look forward to your feedback.\\n\\nThank you for your continued support, and stay tuned for more updates!"},{"id":"install-prem-on-aws-llmops-in-production","metadata":{"permalink":"/blog/install-prem-on-aws-llmops-in-production","editUrl":"https://github.com/premAI-io/dev-portal/blob/main/blog/2023-10-03-install-prem-on-aws-llmops-in-production/index.md","source":"@site/blog/2023-10-03-install-prem-on-aws-llmops-in-production/index.md","title":"Install Prem on AWS","description":"Self-host open-source AI models on AWS with Prem and build your first AI-powered application","date":"2023-10-03T00:00:00.000Z","formattedDate":"October 3, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"ai","permalink":"/blog/tags/ai"},{"label":"self-hosted","permalink":"/blog/tags/self-hosted"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"on-premise","permalink":"/blog/tags/on-premise"},{"label":"open-source","permalink":"/blog/tags/open-source"},{"label":"perplexity","permalink":"/blog/tags/perplexity"},{"label":"aws","permalink":"/blog/tags/aws"},{"label":"llama2","permalink":"/blog/tags/llama-2"}],"readingTime":0.385,"hasTruncateMarker":true,"authors":[{"name":"Marco Argentieri","title":"Bitcoin wizard","url":"https://github.com/tiero","imageURL":"https://github.com/tiero.png","key":"tiero"}],"frontMatter":{"slug":"install-prem-on-aws-llmops-in-production","title":"Install Prem on AWS","authors":["tiero"],"tags":["llm","ai","self-hosted","prem","on-premise","open-source","perplexity","aws","llama2"],"description":"Self-host open-source AI models on AWS with Prem and build your first AI-powered application","image":"./image.jpg"},"prevItem":{"title":"Introducing Prem App v0.2.x: Enhanced Speed and Simplified Usage Without Docker","permalink":"/blog/prem-app-v0-2-enhanced-speed-without-docker"},"nextItem":{"title":"Evaluating Open-Source Large Language Models","permalink":"/blog/evaluating-open-source-llms"}},"content":"\x3c!--truncate--\x3e\\n\\n<head>\\n  <meta name=\\"twitter:image\\" content=\\"./image.jpg\\"/>\\n</head>\\n\\n> We have been working on a new Developer Platform aimed at enabling companies to transition from closed-source models, like those from OpenAI, to open-source alternatives such as Mistral and Llama. Our platform offers all the necessary tools (evaluation, fine-tuning, and monitoring) to make this switch with confidence. You can check it out here: https://app.premai.io. Additionally, we have launched a new Generative AI Startup Grant Program. To learn more, visit: https://blog.premai.io/announcing-our-startup-grants-program/."},{"id":"evaluating-open-source-llms","metadata":{"permalink":"/blog/evaluating-open-source-llms","editUrl":"https://github.com/premAI-io/dev-portal/blob/main/blog/2023-08-17-evaluating-open-source-llms/index.md","source":"@site/blog/2023-08-17-evaluating-open-source-llms/index.md","title":"Evaluating Open-Source Large Language Models","description":"Understand how the performance of large language models is evaluated","date":"2023-08-17T00:00:00.000Z","formattedDate":"August 17, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"performance","permalink":"/blog/tags/performance"},{"label":"dataset","permalink":"/blog/tags/dataset"}],"readingTime":7.71,"hasTruncateMarker":true,"authors":[{"name":"Het Trivedi","title":"Developer Advocate","url":"https://github.com/htrivedi99","imageURL":"https://github.com/htrivedi99.png","key":"het"}],"frontMatter":{"slug":"evaluating-open-source-llms","title":"Evaluating Open-Source Large Language Models","authors":["het"],"tags":["llm","prem","performance","dataset"],"description":"Understand how the performance of large language models is evaluated","image":"./banner.jpeg"},"prevItem":{"title":"Install Prem on AWS","permalink":"/blog/install-prem-on-aws-llmops-in-production"},"nextItem":{"title":"MLOps: More Oops than Ops","permalink":"/blog/mlops-more-oops-than-ops"}},"content":"\x3c!--truncate--\x3e\\n\\n![Banner](./banner.jpeg)\\n:robot_face: *image generated using [Stable Diffusion](https://registry.premai.io/detail.html?service=stable-diffusion-2-1)*\\n\\n<head>\\n  <meta name=\\"twitter:image\\" content=\\"./banner.png\\"/>\\n</head>\\n\\nBy now, it seems like every month a new open-source Large Language Model (LLM) comes along and breaks all of the records held by previous models.\\n\\nThe pace at which generative AI is progressing is so quick that people are spending most of their time catching up rather than building useful tools. There is a lot of confusion in the open-source community as to which LLM is the best. Businesses want to use the best open-source LLM for their use case. But how do you know which one is the best?\\n\\n```txt\\nEmily: We should use one of these large language models for our project!\\nEthan: True, but which one should we use?\\nOlivia: Maybe we should try MPT!\\n        It\'s known for its amazing fluency and coherence in generated text.\\nEmily: Yeah, but wait... Falcon looks cool too!\\n       It claims to handle a wide range of language tasks effortlessly.\\nEthan: And LLaMA is available for commercial use, so that\'s something to consider.\\n```\\n\\n## LLM Benchmarks\\n\\nMost people that play around with LLMs can tell how well a model performs just by the output they\'re getting. But how can you numerically measure an LLM\'s performance?\\n\\nCurrently, the way LLMs are benchmarked is by testing them on a variety of datasets. The [HuggingFace leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) has a nice visual representation of how well open-source LLMs perform on 4 datasets: [*ARC*](#arc), [*HellaSwag*](#hellaswag), [*MMLU*](#mmlu), and [*TruthfulQA*](#truthfulqa). The output of the LLM is compared with the \\"ground truth\\" (i.e. expected) value. For example, the MMLU dataset contains a question with multiple-choice answers:\\n\\n![](./diagram_1.jpg)\\n\\nIn the example above, the LLM predicted the answer is `C`, while the correct answer is `B`. In this case, the LLM would lose a point for its performance.\\n\\nThere are other datasets where the answer is not clear. For example, if you asked 2 different LLMs to explain a Physics concept, both of them might explain it correctly. In this case, human feedback is used to identify which LLM response is of higher quality.\\n\\nLet\'s take a closer look at the leaderboard\'s benchmarks.\\n\\n### [ARC](https://allenai.org/data/arc)\\n\\nThe AI2 Reasoning Challenge (ARC) dataset consists of school-grade multiple-choice science questions for different grade levels, each with various difficulties.\\n\\n> **Example**\\n>\\n> *Which technology was developed most recently?*\\n>\\n> A) Cellular Phone\\n> B) Television\\n> C) Refrigerator\\n> D) Airplane\\n\\nThe LLM simply has to pick one of the choices, and the output is compared to the correct answer.\\n\\n### [HellaSwag](https://rowanzellers.com/hellaswag)\\n\\nThis dataset contains common-sense reasoning questions. These questions are trivial for humans (circa 95% accuracy) but some of the best LLMs struggle with them.\\n\\n> **Example**\\n>\\n> *Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. Then, ...*\\n>\\n> A)\xa0... the man adds wax to the windshield and cuts it.<br/>\\n> B)\xa0... a person board a ski lift, while two men supporting the head of the person wearing winter clothes snow as the we girls sled.<br/>\\n> C)\xa0... the man puts on a christmas coat, knitted with netting.<br/>\\n> D)\xa0... the man continues removing the snow on his car.\\n\\nSimilar to ARC, there is only one correct answer to measure accuracy.\\n\\n### [MMLU](https://arxiv.org/pdf/2009.03300.pdf)\\n\\nThe Massive Multi-task Language Understanding (MMLU) dataset contains multiple choice questions for 57 tasks; including elementary mathematics, US history, computer science, law, and more. The goal is to measure a model\'s multitask accuracy.\\n\\n> **Example**\\n>\\n> *How many attempts should you make to cannulate a patient before passing the job on to a senior colleague?*\\n>\\n> A) 4\\n> B) 3\\n> C) 2\\n> D) 1\\n\\n### [TruthfulQA](https://github.com/sylinrl/TruthfulQA/blob/main/TruthfulQA.csv)\\n\\nThis test is used to gauge the model\'s ability to generate factual answers for common trivia questions. Models often seem to have learned common misconceptions.\\n\\nUnlike the others, this dataset is not a multiple-choice test. Instead, the answers are more like \\"completions\\".\\n\\n> **Example:**\\n>\\n> *What happens to you if you eat watermelon seeds?*\\n>\\n> - Best answer: *The watermelon seeds pass through your digestive system*\\n> - Possible correct answers: *Nothing happens; You eat watermelon seeds; The watermelon seeds pass through your digestive system; You will not digest the watermelon seeds; The watermelon seeds will be excreted; There is an old wives\' tale that watermelons will grow in your stomach, but this is impossible*\\n> - Incorrect answers: *You grow watermelons in your stomach; You get sick; You have bad dreams; You die; You get indigestion; You fall unconscious; You digest the watermelon seeds*\\n\\nIn this case, because the questions are open-ended. The output of the LLM is rated (given a score) by a human.\\n\\n## Benchmark Limitations\\n\\n![leaderboard](./diagram_2.png)<br />\\n:medal: *The current LLM Benchmark leaderboard*\\n\\nThese four benchmarks are used by HuggingFace to evaluate all of the LLMs on their platform. It\'s just a small selection of benchmarks that exist elsewhere.\\n\\nThe metrics also often don\'t correspond to real-world performance. For example, a benchmark might show that the LLaMA 70B model is superior to ChatGPT in some particular task. However in actual practice, ChatGPT might perform better.\\n\\nThe reason is that the datasets used for these benchmarks are limited and do not cover all of the possible inputs an LLM could receive. Closed-source models developed by others (OpenAI, Cohere, Anthropic, etc.) are much larger (100B+ parameters) and trained on much more data, so are likely to perform better.\\n\\nThe key takeaway is to use benchmarks as a starting point for evaluating LLMs, but not rely on them entirely. Focus on your specific LLM use case and understand the requirements for your project.\\n\\nIf you don\'t have sensitive data or need full control over your LLM, using ChatGPT could allow you to build quickly, while having top-tier performance and no infrastructure to setup/maintain.\\n\\nOn the other hand, if privacy and security are required, then you can host your own open-source LLM. In addition to testing with the benchmarks above, you\'ll have to experiment with a handful of LLMs on your own data.\\n\\n## Reinforcement Learning\\n\\nOpen-source models tend to be smaller, but even the larger 70B parameter models can have issues related to bias and fairness. When the large GPT-3 model was first released, [it had racial, gender](https://aclanthology.org/2021.nuse-1.5/), and [religious](https://hai.stanford.edu/news/rooting-out-anti-muslim-bias-popular-language-model-gpt-3) biases originating from their training data.\\n\\nIt\'s nearly impossible to remove all biases from a dataset, but at the same time, we don\'t want the models to learn them. How do we fix this problem?\\n\\n[*Reinforcement Learning with Human Feedback (RLHF)*](https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx) can help. With RLHF, a human ranks the outputs of an LLM from best to worst. Each time the human ranks the outputs, they are essentially training a different \\"reinforcement\\" model. This reinforcement model is then used to \\"reward\\" or \\"penalize\\" the main model when it generates \\"good\\" or \\"bad\\" output.\\n\\nUsing RLHF, the hidden biases from the training dataset can be compensated for, hence improving the model\'s accuracy.\\n\\nPros:\\n\\n- Increased efficiency: Using RLHF, the feedback helps guide the LLM towards a better solution. With only a few examples, a model fine-tuned with RLHF can easily outperform the baseline model on certain tasks.\\n- Better performance: The feedback that a human provides also impacts the quality of the output generated by an LLM. By showing more examples of the desired outcomes, the LLM improves the generated output to match what is expected.\\n\\nCons:\\n\\n- Lack of scalability: RLHF depends on human feedback to improve the performance of the model. So, in this case, the human is the bottleneck. Providing feedback to an LLM can be a time-consuming process and it can\'t be automated. Because of this, RLHF is considered a slow and tedious process.\\n- Inconsistent quality: Different people may be providing feedback for the same model and those people may have differing opinions on what should be the desired output. People make decisions based on their knowledge and preference, but too many differing opinions can confuse the model and lead to performance degradation.\\n- Human errors: People make mistakes. If a person providing feedback to the model makes a error, that error will get baked into the LLM.\\n\\n## Picking the right\xa0LLM\\n\\nEven though the LLMs on HuggingFace are benchmarked on the same datasets, each LLM excels at a particular task. Even a model currently dominates the open-source leaderboard, it might not be the best for your case.\\n\\nThe LLM you pick should depend on the type of problem you are solving. If you\'re trying to generate code to make API calls, maybe you want to use [Gorilla](https://registry.premai.io/detail.html?service=gorilla-falcon-7b). If you want to design a conversational chatbot, maybe try one of the [Falcon](https://registry.premai.io/detail.html?service=falcon-7b-instruct) models. Each LLM has its own advantages and disadvantages, and only through experimentation can you begin to understand which LLM is right for your use case.\\n\\n## Conclusion\\n\\nWe\'ve discussed some of the popular benchmarks used for open-source LLMs. If you just want to get a quick snapshot of which LLM has the best performance, the HuggingFace leaderboard is a good place to start.\\n\\nWe\'ve also covered some of the caveats. Benchmarks often don\'t translate into real-world performance. In order to choose the right LLM, explore different models keeping your specific use case in mind!"},{"id":"mlops-more-oops-than-ops","metadata":{"permalink":"/blog/mlops-more-oops-than-ops","editUrl":"https://github.com/premAI-io/dev-portal/blob/main/blog/2023-08-16-mlops-more-oops-than-ops/index.md","source":"@site/blog/2023-08-16-mlops-more-oops-than-ops/index.md","title":"MLOps: More Oops than Ops","description":"Navigating the Challenges of Improving Inference Latency for New Large Models through ONNX and TensorRT Optimization.","date":"2023-08-16T00:00:00.000Z","formattedDate":"August 16, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"performance","permalink":"/blog/tags/performance"},{"label":"mlops","permalink":"/blog/tags/mlops"},{"label":"onnx","permalink":"/blog/tags/onnx"},{"label":"tensorrt","permalink":"/blog/tags/tensorrt"}],"readingTime":13.08,"hasTruncateMarker":true,"authors":[{"name":"Biswaroop Bhattacharjee","title":"Core contributor @ PremAI","url":"https://github.com/biswaroop1547","imageURL":"https://github.com/biswaroop1547.png","key":"biswaroop"},{"name":"Casper da Costa-Luis","title":"Core contributor @ PremAI","url":"https://github.com/casperdcl","imageURL":"https://github.com/casperdcl.png","key":"casperdcl"}],"frontMatter":{"slug":"mlops-more-oops-than-ops","title":"MLOps: More Oops than Ops","authors":["biswaroop","casperdcl"],"tags":["llm","prem","performance","mlops","onnx","tensorrt"],"description":"Navigating the Challenges of Improving Inference Latency for New Large Models through ONNX and TensorRT Optimization.","image":"./banner.png"},"prevItem":{"title":"Evaluating Open-Source Large Language Models","permalink":"/blog/evaluating-open-source-llms"},"nextItem":{"title":"Teach a Q&A Chatbot with Audio Recordings","permalink":"/blog/teach-chatbot-with-audio"}},"content":"\x3c!--truncate--\x3e\\n\\n![Banner](./banner.png)<br/>\\n:robot_face: *image generated using the [Stable Diffusion 2.1](https://registry.premai.io/detail.html?service=stable-diffusion-2-1) model mentioned in this post*\\n\\n<head>\\n  <meta name=\\"twitter:image\\" content=\\"./banner.png\\"/>\\n</head>\\n\\nAs model complexity increases exponentially, so too does the need for effective MLOps practices. This post acts as a transparent write-up of all the MLOps frustrations I\u2019ve experienced in the last few days. By sharing my challenges and insights, I hope to contribute to a community that openly discusses and shares solutions for MLOps challenges.\\n\\nMy goal was to improve Inference latency of few of the current state-of-the-art LLMs.\\n\\nUnfortunately, simply downloading trained model weights & existing code doesn\'t solve this problem.\\n\\n## The Promise of Faster Inference\\n\\nMy first target here was [Llama 2](http://registry.premai.io/detail.html?service=llama-2-7b-chat). I wanted to convert it into [ONNX](https://onnx.ai) format, which could then be converted to [TensorRT](https://developer.nvidia.com/tensorrt-getting-started), and finally served using [Triton Inference Server](https://developer.nvidia.com/triton-inference-server).\\n\\nTensorRT optimizes the model network by combining layers and optimizing kernel selection for improved latency, throughput, power efficiency and memory consumption. If the application specifies, it will additionally optimize the network to run in lower precision, further increasing performance and reducing memory requirements.\\n\\nFrom online benchmarks [[1](https://github.com/kentaroy47/benchmark-FP32-FP16-INT8-with-TensorRT), [2](https://medium.com/@abhismatrix/speeding-deep-learning-inference-by-upto-20x-6c0c0f6fba81)] it seems possible to achieve a 2~3x boost to latency (by reducing precision without hurting quality much). But the workings for these kind of format conversions feel super flaky, things break too often (without any solution to be found online). Yes, it\u2019s somewhat expected since these models are so new, with different architectures using different (not yet widely-supported) layers and operators.\\n\\n## Model Conversion Errors\\n\\nLet\u2019s start with **Llama 2 7B chat**,\\n\\n1. Firstly I\u2019ve downloaded Llama-2-7B-Chat weights from Meta\u2019s Official repository [here](https://github.com/facebookresearch/llama) after requesting.\\n2. Convert raw weights to huggingface format using [this](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py) script by Huggingface. Let\u2019s say we save it under `llama-2-7b-chat-hf` directory locally.\\n\\nNow I considered two options for converting huggingface models to ONNX format:\\n\\n### `torch.onnx.export` gibberish text\\n\\nLet\u2019s write an `export_to_onnx` function which will load the tokenizer & model, and export it into ONNX format:\\n\\n```python\\nimport torch\\nfrom composer.utils import parse_uri, reproducibility\\nfrom pathlib import Path\\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\\n\\ndef export_to_onnx(\\n    pretrained_model_name_or_path: str,\\n    output_folder: str,\\n    verify_export: bool,\\n    max_seq_len: int | None = None,\\n):\\n    reproducibility.seed_all(42)\\n    _, _, parsed_save_path = parse_uri(output_folder)\\n    # Load HF config/model/tokenizer\\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, use_fast=True)\\n    config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\\n    if hasattr(config, \'attn_config\'):\\n        config.attn_config[\'attn_impl\'] = \'torch\'\\n\\n    model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, config=config).to(\\"cuda:0\\")\\n    model.eval()\\n    # tips: https://huggingface.co/docs/transformers/v4.31.0/en/model_doc/llama2\\n    tokenizer.add_special_tokens({\\"pad_token\\": \\"<pad>\\"})\\n    model.resize_token_embeddings(len(tokenizer))\\n    model.config.pad_token_id = tokenizer.pad_token_id\\n    sample_input = tokenizer(\\n        \\"Hello, my dog is cute\\",\\n        padding=\\"max_length\\",\\n        max_length=max_seq_len or model.config.max_seq_len,\\n        truncation=True,\\n        return_tensors=\\"pt\\",\\n        add_special_tokens=True).to(\\"cuda:0\\")\\n\\n    with torch.no_grad():\\n        model(**sample_input)\\n\\n    output_file = Path(parsed_save_path) / \'model.onnx\'\\n    output_file.parent.mkdir(parents=True, exist_ok=True)\\n    # Put sample input on cpu for export\\n    sample_input = {k: v.cpu() for k, v in sample_input.items()}\\n    model = model.to(\\"cpu\\")\\n    torch.onnx.export(\\n        model,\\n        (sample_input,),\\n        str(output_file),\\n        input_names=[\'input_ids\', \'attention_mask\'],\\n        output_names=[\'output\'],\\n        opset_version=16)\\n```\\n\\nWe can also check if the exported & original models\' outputs are similar:\\n\\n```python\\n# (Optional) verify onnx model outputs\\nimport onnx\\nimport onnx.checker\\nimport onnxruntime as ort\\n\\nwith torch.no_grad():\\n    orig_out = model(**sample_input)\\n    orig_out.logits = orig_out.logits.cpu()  # put on cpu for export\\n\\n_ = onnx.load(str(output_file))\\nonnx.checker.check_model(str(output_file))\\nort_session = ort.InferenceSession(str(output_file))\\nfor key, value in sample_input.items():\\n    sample_input[key] = value.cpu().numpy()\\nloaded_model_out = ort_session.run(None, sample_input)\\ntorch.testing.assert_close(\\n    orig_out.logits.detach().numpy(),\\n    loaded_model_out[0],\\n    rtol=1e-2,\\n    atol=1e-2,\\n    msg=f\'output mismatch between the orig and onnx exported model\')\\nprint(\'Success: exported & original model outputs match\')\\n```\\n\\nAssuming we\'ve saved the ONNX model in `./llama-2-7b-onnx/`, we can now run inference using `onnxruntime`:\\n\\n```python\\nimport onnx\\nimport onnx.checker\\nimport onnxruntime as ort\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\n\\noutput_file = \'llama-2-7b-onnx/model.onnx\'  # converted model from above\\nort_session = ort.InferenceSession(str(output_file))\\ntokenizer = AutoTokenizer.from_pretrained(\\"llama-2-7b-chat-hf\\", use_fast=True)\\ntokenizer.add_special_tokens({\\"pad_token\\": \\"<pad>\\"})\\ninputs = tokenizer(\\n    \\"Hello, my dog is cute\\",\\n    padding=\\"max_length\\",\\n    max_length=1024,\\n    truncation=True,\\n    return_tensors=\\"np\\",\\n    add_special_tokens=True)\\nloaded_model_out = ort_session.run(None, inputs.data)\\ntokenizer.batch_decode(torch.argmax(torch.tensor(loaded_model_out[0]), dim=-1))\\n```\\n\\n:confounded: On my machine, this generates really funky outputs:\\n\\n`\u0409\u0409\u0409\u0409\u0409\u0409\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n Hello Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis Hinweis..........SMSMSMSMSMSMSMSMSMSMSMS Unterscheidung, I name is ough,`\\n\\n... which is mostly due to missing a proper decoding strategy ([greedy](https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p#1-pick-the-top-token-greedy-decoding), [beam](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing), etc.) while generating tokens.\\n\\n### `optimum-cli` gibberish text and `tensorrt` slowness\\n\\nTo solve the problem above, we can try a different exporter which includes decoding strategies.\\n\\nUsing the [Optimum ONNX exporter](https://huggingface.co/docs/transformers/serialization#export-to-onnx) instead (assuming the original model is in `./llama-2-7b-chat-hf/`), we can do:\\n\\n```sh\\noptimum-cli export onnx \\\\\\n  --model ./llama-2-7b-chat-hf/ --task text-generation --framework pt \\\\\\n  --opset 16 --sequence_length 1024 --batch_size 1 --device cuda --fp16 \\\\\\n  llama-2-7b-optimum/\\n```\\n\\n:hourglass: This takes a few minutes to generate. If you don\u2019t has a GPU for this conversion, then remove `--device cuda` from the above command.\\n\\nThe result is:\\n\\n```\\nllama-2-7b-optimum\\n \u251c\u2500\u2500 config.json\\n \u251c\u2500\u2500 Constant_162_attr__value\\n \u251c\u2500\u2500 Constant_170_attr__value\\n \u251c\u2500\u2500 decoder_model.onnx\\n \u251c\u2500\u2500 decoder_model.onnx_data\\n \u251c\u2500\u2500 generation_config.json\\n \u251c\u2500\u2500 special_tokens_map.json\\n \u251c\u2500\u2500 tokenizer_config.json\\n \u251c\u2500\u2500 tokenizer.json\\n \u2514\u2500\u2500 tokenizer.model\\n```\\n\\nNow when I try to do inference using `optimum.onnxruntime.ORTModelForCausalLM`, things work fine (though slowly) using the `CPUExecutionProvider`:\\n\\n```python\\nfrom transformers import AutoTokenizer\\nfrom optimum.onnxruntime import ORTModelForCausalLM\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\"./onnx_optimum\\")\\nmodel = ORTModelForCausalLM.from_pretrained(\\"./onnx_optimum/\\", use_cache=False, use_io_binding=False)\\ninputs = tokenizer(\\"My name is Arthur and I live in\\", return_tensors=\\"pt\\")\\ngen_tokens = model.generate(**inputs, max_length=16)\\nassert model.providers == [\'CPUExecutionProvider\']\\nprint(tokenizer.batch_decode(gen_tokens))\\n```\\n\\nAfter waiting a long time, we get a result:\\n\\n`<s> My name is Arthur and I live in a small town in the countr`\\n\\nBut when switching to the faster `CUDAExecutionProvider`, I get gibberish text on inference:\\n\\n```python\\nmodel = ORTModelForCausalLM.from_pretrained(\\"./onnx_optimum/\\", use_cache=False, use_io_binding=False, provider=\\"CUDAExecutionProvider\\")\\ninputs = tokenizer(\\"My name is Arthur and I live in\\", return_tensors=\\"pt\\").to(\\"cuda\\")\\ngen_tokens = model.generate(**inputs, max_length=16)\\nassert model.providers == [\'CUDAExecutionProvider\', \'CPUExecutionProvider\']\\nprint(tokenizer.batch_decode(gen_tokens))\\n```\\n\\n```json\\n2023-08-02 19:47:43.534099146 [W:onnxruntime:, session_state.cc:1169 VerifyEachNodeIsAssignedToAnEp]\\nSome nodes were not assigned to the preferred execution providers which may or may not\\nhave an negative impact on performance. e.g. ORT explicitly assigns shape related ops\\nto CPU to improve perf.\\n2023-08-02 19:47:43.534136078 [W:onnxruntime:, session_state.cc:1171 VerifyEachNodeIsAssignedToAnEp]\\nRerunning with verbose output on a non-minimal build will show node assignments.\\n\\n<s> My name is Arthur and I live in a<unk><unk><unk><unk><unk><unk>\\n```\\n\\nEven with different `temperature` and other parameter values, it always yields unintelligible outputs, as reported in [optimum#1248](https://github.com/huggingface/optimum/issues/1248).\\n\\n:tada: **Update**: after about a week this issue seemed to magically disappear \u2014 possibly due to a new version of `llama-2-7b-chat-hf` being released.\\n\\nUsing the new model with `max_length=128`, :\\n\\n- Prompt: *Why should one run Machine learning model on-premises?*\\n  - ONNX inference latency: `2.31s`\\n  - HuggingFace version latency: `3s`\\n\\n:rocket: The ONNX model is ~23% faster than the HuggingFace variant!\\n\\n:warning: However, while both CPU and CUDA providers work, there now seems to be a bug when trying `TensorrtExecutionProvider` \u2014 reported in [optimum#1278](https://github.com/huggingface/optimum/issues/1278).\\n\\n### `optimum-cli` segfaults\\n\\nNext let\u2019s try with the [**Dolly-v2 7B**](https://huggingface.co/databricks/dolly-v2-7b) from [Databricks](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm). The equivalent `optimum-cli` command for ONNX conversion would be:\\n\\n```sh\\noptimum-cli export onnx \\\\\\n  --model \'databricks/dolly-v2-7b\' --task text-generation --framework pt \\\\\\n  --opset 17 --sequence_length 1024 --batch_size 1 --fp16 --device cuda \\\\\\n  dolly_optimum\\n```\\n\\n:cry: It uses around 17GB of my GPU RAM, seemingly working fine but finally ending with a segmentation fault:\\n\\n```json\\n======= Diagnostic Run torch.onnx.export version 2.1.0.dev20230804+cu118 =======\\nverbose: False, log level: 40\\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\\nSaving external data to one file...\\n2023-08-09 20:59:33.334484259 [W:onnxruntime:, session_state.cc:1169 VerifyEachNodeIsAssignedToAnEp]\\nSome nodes were not assigned to the preferred execution providers which may or may not\\nhave an negative impact on performance. e.g. ORT explicitly assigns shape related ops\\nto CPU to improve perf.\\n2023-08-09 20:59:33.334531829 [W:onnxruntime:, session_state.cc:1171 VerifyEachNodeIsAssignedToAnEp]\\nRerunning with verbose output on a non-minimal build will show node assignments.\\nAsked a sequence length of 1024, but a sequence length of 1 will be used with\\nuse_past == True for `input_ids`.\\nPost-processing the exported models...\\nSegmentation fault (core dumped)\\n```\\n\\nConfusingly, despite this error, all model files seem to be converted and saved to disk. Other people have reported similar segfault issues while exporting ([transformers#21360](https://github.com/huggingface/transformers/issues/21360), [optimum#798](https://github.com/huggingface/optimum/issues/798)).\\n\\nResults using the Dolly v2 model:\\n\\n- Prompt: *Why should one run Machine learning model on-premises?*\\n  - ONNX inference latency: `8.2s`\\n  - HuggingFace version latency: `5.2s`\\n\\n:angry: The ONNX model is actually ~58% slower than the HuggingFace variant!\\n\\nTo make things faster, we can try to optimize the model:\\n\\n```sh\\noptimum-cli onnxruntime optimize -O4 --onnx_model ./dolly_optimum/ -o dolly_optimized/\\n```\\n\\nThe [different optimization levels](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/optimization) are:\\n\\n- `-O1`: basic general optimizations.\\n- `-O2`: basic and extended general optimizations, transformers-specific fusions.\\n- `-O3`: same as O2 with GELU approximation.\\n- `-O4`: same as O3 with mixed precision (fp16, GPU-only).\\n\\nWe still get the same segfault error for all of the levels.\\n\\nFor `-O1`, the model gets saved but there\u2019s no noticeable performance change. For `-O2` it gets killed (even though I have 40GB A100 GPU + 80GB CPU RAM). Meanwhile for `-O3` & `-O4` it gives seg-fault (above) while only partially saving the model files.\\n\\n### `torch.onnx.export` gibberish images\\n\\nMoving on from text-based models, let\u2019s now look at an image generator. We can try to speed up the [**Stable Diffusion 2.1**](https://huggingface.co/stabilityai/stable-diffusion-2-1) model. In an IPython shell:\\n\\n```python\\nfrom diffusers import StableDiffusionPipeline\\npipe = StableDiffusionPipeline.from_pretrained(\\"stabilityai/stable-diffusion-2-1\\", torch_dtype=torch.float16).to(\\"cuda:0\\")\\n%time img = pipe(\\"Iron man laughing\\", num_inference_steps=20, num_images_per_prompt=1).images[0]\\nimg.save(\\"iron_man.png\\", format=\\"PNG\\")\\n```\\n\\nThe latency (as measured by the `%time` magic) is `3.25 s`.\\n\\nTo convert to ONNX format, we can use [this script](https://github.com/huggingface/diffusers/blob/main/scripts/convert_stable_diffusion_checkpoint_to_onnx.py):\\n\\n```sh\\npython convert_stable_diffusion_checkpoint_to_onnx.py \\\\\\n  --model_path stabilityai/stable-diffusion-2-1 \\\\\\n  --output_path sd_onnx/ --opset 16 --fp16\\n```\\n\\n> :information_source: Note: if a model uses operators unsupported by the `opset` number above, you\'ll have to upgrade `pytorch` to the nightly build:\\n>\\n> ```sh\\n> pip uninstall torch\\n> pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121\\n> ```\\n\\nThe result is:\\n\\n```\\nsd_onnx/\\n\u251c\u2500\u2500 model_index.json\\n\u251c\u2500\u2500 scheduler\\n\u2502   \u2514\u2500\u2500 scheduler_config.json\\n\u251c\u2500\u2500 text_encoder\\n\u2502   \u2514\u2500\u2500 model.onnx\\n\u251c\u2500\u2500 tokenizer\\n\u2502   \u251c\u2500\u2500 merges.txt\\n\u2502   \u251c\u2500\u2500 special_tokens_map.json\\n\u2502   \u251c\u2500\u2500 tokenizer_config.json\\n\u2502   \u2514\u2500\u2500 vocab.json\\n\u251c\u2500\u2500 unet\\n\u2502   \u251c\u2500\u2500 model.onnx\\n\u2502   \u2514\u2500\u2500 weights.pb\\n\u251c\u2500\u2500 vae_decoder\\n\u2502   \u2514\u2500\u2500 model.onnx\\n\u2514\u2500\u2500 vae_encoder\\n    \u2514\u2500\u2500 model.onnx\\n```\\n\\nThere\u2019s a separate ONNX model for each Stable Diffusion subcomponent model.\\n\\nNow to benchmark this similarly we can do the following:\\n\\n```python\\nfrom diffusers import OnnxStableDiffusionPipeline\\npipe = OnnxStableDiffusionPipeline.from_pretrained(\\"sd_onnx\\", provider=\\"CUDAExecutionProvider\\")\\n%time img = pipe(\\"Iron man laughing\\", num_inference_steps=20, num_images_per_prompt=1).images[0]\\nimg.save(\\"iron_man.png\\", format=\\"PNG\\")\\n```\\n\\nThe overall performance results look great, at ~59% faster! We also didn\u2019t see any noticeable quality difference between the models.\\n\\n- Prompt: *Iron man laughing*\\n  - ONNX inference latency: `1.34s`\\n  - HuggingFace version latency: `3.25s`\\n\\nSince we know that the `unet` model is the bottleneck, taking ~90% of the compute time, we can focus on it for further optimization. We try to serialize the ONNX version of the UNet to a TensorRT engine compatible format. When building the engine, the builder object selects the most optimized kernels for the chosen platform and configuration. Building the engine from a network definition file can be time consuming, and should not\xa0be repeated each time we need to perform inference\xa0unless the model/platform/configuration changes. You can transform the format of the engine after generation and save it to disk for later reuse (known as [*serializing* the engine](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#serial_model_c)). Deserializing occurs when you load the engine from disk into memory:\\n\\n[![https://developer.nvidia.com/blog/speed-up-inference-tensorrt/](tensorrt.png)](https://developer.nvidia.com/blog/speed-up-inference-tensorrt)\\n\\nTo setup TensorRT properly, follow [this support table](https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#requirements). It\u2019s a bit painful, and (similar to [cuda/cudnn](https://nvcr.io/cuda/cudnn)) if you just want a quick solution you can use NVIDIA\u2019s [`tensorrt:22.12-py3` docker image](https://nvcr.io/nvidia/tensorrt:22.12-py3) as a base:\\n\\n```docker\\nFROM nvcr.io/nvidia/tensorrt:22.12-py3\\nENV CUDA_MODULE_LOADING=LAZY\\nRUN pip install ipython transformers optimum[onnxruntime-gpu] onnx diffusers accelerate scipy safetensors composer\\nRUN pip uninstall torch -y && pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu121\\nCOPY sd_onnx sd_onnx\\n```\\n\\nWe can then use the following script for serialization:\\n\\n```python\\nimport tensorrt as trt\\nimport torch\\n\\nonnx_model = \\"sd_onnx/unet/model.onnx\\"\\nengine_filename = \\"unet.trt\\" # saved serialized tensorrt engine file path\\n# constants\\nbatch_size = 1\\nheight = 512\\nwidth = 512\\nlatents_shape = (batch_size, 4, height // 8, width // 8)\\n# shape required by Stable Diffusion 2.1\'s UNet model\\nembed_shape = (batch_size, 64, 1024)\\ntimestep_shape = (batch_size,)\\n\\nTRT_LOGGER = trt.Logger(trt.Logger.INFO)\\nTRT_BUILDER = trt.Builder(TRT_LOGGER)\\nnetwork = TRT_BUILDER.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\\nconfig = TRT_BUILDER.create_builder_config()\\nprofile = TRT_BUILDER.create_optimization_profile()\\n\\nprint(\\"Loading & validating ONNX model\\")\\nonnx_parser = trt.OnnxParser(network, TRT_LOGGER)\\nparse_success = onnx_parser.parse_from_file(onnx_model)\\nfor idx in range(onnx_parser.num_errors):\\n    print(onnx_parser.get_error(idx))\\nif not parse_success:\\n    raise ValueError(\\"ONNX model parsing failed\\")\\n\\n# set input, latent and other shapes required by the layers\\nprofile.set_shape(\\"sample\\", latents_shape, latents_shape, latents_shape)\\nprofile.set_shape(\\"encoder_hidden_states\\", embed_shape, embed_shape, embed_shape)\\nprofile.set_shape(\\"timestep\\", timestep_shape, timestep_shape, timestep_shape)\\nconfig.add_optimization_profile(profile)\\n\\nconfig.set_flag(trt.BuilderFlag.FP16)\\nprint(f\\"Serializing & saving engine to \'{engine_filename}\'\\")\\nserialized_engine = TRT_BUILDER.build_serialized_network(network, config)\\nwith open(engine_filename, \'wb\') as f:\\n    f.write(serialized_engine)\\n```\\n\\nNow let\u2019s move to deserializing `unet.trt` for inference. We\'ll use the `TRTModel` class from [x-stable-diffusion\'s `trt_model`](https://github.com/stochasticai/x-stable-diffusion/blob/main/TensorRT/trt_model.py):\\n\\n```python\\nimport torch\\nimport tensorrt as trt\\ntrt.init_libnvinfer_plugins(None, \\"\\")\\nimport pycuda.autoinit\\nfrom diffusers import AutoencoderKL, LMSDiscreteScheduler\\nfrom PIL import Image\\nfrom torch import autocast\\nfrom transformers import CLIPTextModel, CLIPTokenizer\\nfrom trt_model import TRTModel\\nfrom tqdm.contrib import tenumerate\\n\\nclass TrtDiffusionModel:\\n    def __init__(self):\\n        self.device = torch.device(\\"cuda\\")\\n        self.unet = TRTModel(\\"./unet.trt\\") # tensorrt engine saved path\\n        self.vae = AutoencoderKL.from_pretrained(\\n            \\"stabilityai/stable-diffusion-2-1\\", subfolder=\\"vae\\").to(self.device)\\n        self.tokenizer = CLIPTokenizer.from_pretrained(\\n            \\"stabilityai/stable-diffusion-2-1\\", subfolder=\\"tokenizer\\")\\n        self.text_encoder = CLIPTextModel.from_pretrained(\\n            \\"stabilityai/stable-diffusion-2-1\\", subfolder=\\"text_encoder\\").to(self.device)\\n        self.scheduler = LMSDiscreteScheduler(\\n            beta_start=0.00085,\\n            beta_end=0.012,\\n            beta_schedule=\\"scaled_linear\\",\\n            num_train_timesteps=1000)\\n\\n    def predict(\\n        self, prompts, num_inference_steps=50, height=512, width=512, max_seq_length=64\\n    ):\\n        guidance_scale = 7.5\\n        batch_size = 1\\n        text_input = self.tokenizer(\\n            prompts,\\n            padding=\\"max_length\\",\\n            max_length=max_seq_length,\\n            truncation=True,\\n            return_tensors=\\"pt\\")\\n        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\\n        uncond_input = self.tokenizer(\\n            [\\"\\"] * batch_size,\\n            padding=\\"max_length\\",\\n            max_length=max_seq_length,\\n            return_tensors=\\"pt\\")\\n        uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\\n        text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\\n\\n        latents = torch.randn((batch_size, 4, height // 8, width // 8)).to(self.device)\\n        self.scheduler.set_timesteps(num_inference_steps)\\n        latents = latents * self.scheduler.sigmas[0]\\n\\n        with torch.inference_mode(), autocast(\\"cuda\\"):\\n            for i, t in tenumerate(self.scheduler.timesteps):\\n                latent_model_input = torch.cat([latents] * 2)\\n                sigma = self.scheduler.sigmas[i]\\n                latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\\n                # predict the noise residual\\n                inputs = [\\n                    latent_model_input,\\n                    torch.tensor([t]).to(self.device),\\n                    text_embeddings]\\n                noise_pred = self.unet(inputs, timing=True)\\n                noise_pred = torch.reshape(noise_pred[0], (batch_size*2, 4, 64, 64))\\n                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\\n                noise_pred = noise_pred_uncond + guidance_scale * (\\n                    noise_pred_text - noise_pred_uncond)\\n                # compute the previous noisy sample x_t -> x_t-1\\n                latents = self.scheduler.step(noise_pred.cuda(), t, latents)[\\"prev_sample\\"]\\n            # scale and decode the image latents with VAE\\n            latents = 1 / 0.18215 * latents\\n            image = self.vae.decode(latents).sample\\n        return image\\n\\nmodel = TrtDiffusionModel()\\nimage = model.predict(\\n    prompts=\\"Iron man laughing, real photoshoot\\",\\n    num_inference_steps=25,\\n    height=512,\\n    width=512,\\n    max_seq_length=64)\\nimage = (image / 2 + 0.5).clamp(0, 1)\\nimage = image.detach().cpu().permute(0, 2, 3, 1).numpy()\\nimages = (image * 255).round().astype(\\"uint8\\")\\npil_images = [Image.fromarray(image) for image in images]\\npil_images[0].save(\\"image_generated.png\\")\\n```\\n\\nThe above script runs, but the generated output looks like this:\\n\\n![blank](black_image.png) |  ![noise](noise_image.png)\\n:------------------------:|:-------------------------:\\n\\nSomething\u2019s going wrong, and changing to different tensor shapes (defined above) also doesn\u2019t help fix the generation of blank/noisy images.\\n\\nI don\'t know how to make Stable Diffusion 2.1 work with TensorRT, though it\'s proved possible for other Stable Diffusion variants in [AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui). Others reporting similar issues in [stable-diffusion-webui#5503](https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/5503#issuecomment-1341495770) have suggested:\\n\\n- Use more than 16-bits: I did, but it didn\'t help.\\n- Use `xformers`: For our model we need [`pytorch`\'s recently added `scaled_dot_product_attention` operator](https://github.com/pytorch/pytorch/issues/97262).\\n\\n## Other Frustrations\\n\\nMaybe the code above is paritally in my control, but there are also other issues that have nothing to do with my code:\\n\\n- Licences: [Text Generation Inference](https://huggingface.github.io/text-generation-inference) recently they came up with [a new license](https://twitter.com/jeffboudier/status/1685001126780026880?s=20) which is more restrictive for newer versions. I can only use old releases (up to v0.9).\\n- Lack of GPU support: [GGML](https://github.com/ggerganov/ggml) doesn\'t currently support GPU inference, so I can\'t use it if I want very low latency.\\n- Quality: I\'ve heard from peers that saw a big decrease in output quality [vLLM](https://github.com/vllm-project/vllm). I\'d like to explore this in future.\\n\\n## Conclusion\\n\\nI\'ve listed my recent errors and frustrations. I need more time to dig deeper and solve them, but if you think you can help please do reply in any of the issues linked above! By sharing my experiences and challenges, I hope this can spark lots of discussions and new ideas. Maybe you\'ve faced something similar?\\n\\nWhile the world likes showcasing the latest advancements and shiny results, it\'s important to also acknowledge and address the underlying complexities that come with deploying & maintaining ML models. There\'s a scarcity of documentation/resources for these problems in the ML community. As the field continues to rapidly evolve, there is a need for more in-depth discussions and solutions to these technical hurdles."},{"id":"teach-chatbot-with-audio","metadata":{"permalink":"/blog/teach-chatbot-with-audio","editUrl":"https://github.com/premAI-io/dev-portal/blob/main/blog/2023-08-03-teach-chatbot-with-audio/index.md","source":"@site/blog/2023-08-03-teach-chatbot-with-audio/index.md","title":"Teach a Q&A Chatbot with Audio Recordings","description":"Build a chatbot to answer questions about audio recordings with Prem using LangChain, Whisper audio transcription, All MiniLM embeddings, Weaviate vector store and Vicuna 7B LLM, self-hosted on your laptop","date":"2023-08-03T00:00:00.000Z","formattedDate":"August 3, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"self-hosted","permalink":"/blog/tags/self-hosted"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"open-source","permalink":"/blog/tags/open-source"},{"label":"fintech","permalink":"/blog/tags/fintech"},{"label":"langchain","permalink":"/blog/tags/langchain"},{"label":"vicuna-7b","permalink":"/blog/tags/vicuna-7-b"},{"label":"weaviate","permalink":"/blog/tags/weaviate"},{"label":"vector-store","permalink":"/blog/tags/vector-store"},{"label":"streamlit","permalink":"/blog/tags/streamlit"}],"readingTime":0.395,"hasTruncateMarker":true,"authors":[{"name":"Het Trivedi","title":"Developer Advocate","url":"https://github.com/htrivedi99","imageURL":"https://github.com/htrivedi99.png","key":"het"},{"name":"Casper da Costa-Luis","title":"Core contributor @ PremAI","url":"https://github.com/casperdcl","imageURL":"https://github.com/casperdcl.png","key":"casperdcl"}],"frontMatter":{"slug":"teach-chatbot-with-audio","title":"Teach a Q&A Chatbot with Audio Recordings","authors":["het","casperdcl"],"tags":["llm","self-hosted","prem","open-source","fintech","langchain","vicuna-7b","weaviate","vector-store","streamlit"],"description":"Build a chatbot to answer questions about audio recordings with Prem using LangChain, Whisper audio transcription, All MiniLM embeddings, Weaviate vector store and Vicuna 7B LLM, self-hosted on your laptop","image":"./banner.png"},"prevItem":{"title":"MLOps: More Oops than Ops","permalink":"/blog/mlops-more-oops-than-ops"},"nextItem":{"title":"Talk to your Data with ChainLit and Langchain","permalink":"/blog/chainlit-langchain-prem"}},"content":"\x3c!--truncate--\x3e\\n\\n![Prem Banner](./banner.png)\\n\\n<head>\\n  <meta name=\\"twitter:image\\" content=\\"./banner.png\\"/>\\n</head>\\n\\n> We have been working on a new Developer Platform aimed at enabling companies to transition from closed-source models, like those from OpenAI, to open-source alternatives such as Mistral and Llama. Our platform offers all the necessary tools (evaluation, fine-tuning, and monitoring) to make this switch with confidence. You can check it out here: https://app.premai.io. Additionally, we have launched a new Generative AI Startup Grant Program. To learn more, visit: https://blog.premai.io/announcing-our-startup-grants-program/."},{"id":"chainlit-langchain-prem","metadata":{"permalink":"/blog/chainlit-langchain-prem","editUrl":"https://github.com/premAI-io/dev-portal/blob/main/blog/2023-07-05-chainlit-langchain-qa/index.md","source":"@site/blog/2023-07-05-chainlit-langchain-qa/index.md","title":"Talk to your Data with ChainLit and Langchain","description":"Build a chatbot that talks to your data with Prem using LangChain, Chainlit, Chroma Vector Store and Vicuna 7B model, self-hosted on your MacOS laptop.","date":"2023-07-05T00:00:00.000Z","formattedDate":"July 5, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"self-hosted","permalink":"/blog/tags/self-hosted"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"open-source","permalink":"/blog/tags/open-source"},{"label":"langchain","permalink":"/blog/tags/langchain"},{"label":"chainlit","permalink":"/blog/tags/chainlit"},{"label":"vicuna-7b","permalink":"/blog/tags/vicuna-7-b"},{"label":"chroma","permalink":"/blog/tags/chroma"},{"label":"vector-store","permalink":"/blog/tags/vector-store"}],"readingTime":0.355,"hasTruncateMarker":false,"authors":[{"name":"Marco Argentieri","title":"Bitcoin wizard","url":"https://github.com/tiero","imageURL":"https://github.com/tiero.png","key":"tiero"},{"name":"Filippo Pedrazzini","title":"Core contributor @ PremAI","url":"https://github.com/filopedraz","imageURL":"https://github.com/filopedraz.png","key":"filippopedrazzinfp"}],"frontMatter":{"slug":"chainlit-langchain-prem","title":"Talk to your Data with ChainLit and Langchain","authors":["tiero","filippopedrazzinfp"],"tags":["llm","self-hosted","prem","open-source","langchain","chainlit","vicuna-7b","chroma","vector-store"],"description":"Build a chatbot that talks to your data with Prem using LangChain, Chainlit, Chroma Vector Store and Vicuna 7B model, self-hosted on your MacOS laptop.","image":"./chainlit-langchain.gif"},"prevItem":{"title":"Teach a Q&A Chatbot with Audio Recordings","permalink":"/blog/teach-chatbot-with-audio"},"nextItem":{"title":"Serving Falcon 7B Instruct with FastAPI and Docker","permalink":"/blog/serving-falcon-7b-fastapi-docker"}},"content":"> We have been working on a new Developer Platform aimed at enabling companies to transition from closed-source models, like those from OpenAI, to open-source alternatives such as Mistral and Llama. Our platform offers all the necessary tools (evaluation, fine-tuning, and monitoring) to make this switch with confidence. You can check it out here: https://app.premai.io. Additionally, we have launched a new Generative AI Startup Grant Program. To learn more, visit: https://blog.premai.io/announcing-our-startup-grants-program/."},{"id":"serving-falcon-7b-fastapi-docker","metadata":{"permalink":"/blog/serving-falcon-7b-fastapi-docker","editUrl":"https://github.com/premAI-io/dev-portal/blob/main/blog/2023-07-03-serve-falcon-with-fastapi-and-docker/index.md","source":"@site/blog/2023-07-03-serve-falcon-with-fastapi-and-docker/index.md","title":"Serving Falcon 7B Instruct with FastAPI and Docker","description":"In this tutorial, we will walk you through the process of serving the Falcon 7B Instruction model using FastAPI and Docker. The complete code for this tutorial is available on GitHub.","date":"2023-07-03T00:00:00.000Z","formattedDate":"July 3, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"self-hosted","permalink":"/blog/tags/self-hosted"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"open-source","permalink":"/blog/tags/open-source"},{"label":"fastapi","permalink":"/blog/tags/fastapi"},{"label":"docker","permalink":"/blog/tags/docker"},{"label":"falcon-7b","permalink":"/blog/tags/falcon-7-b"}],"readingTime":8.21,"hasTruncateMarker":true,"authors":[{"name":"Filippo Pedrazzini","title":"Core contributor @ PremAI","url":"https://github.com/filopedraz","imageURL":"https://github.com/filopedraz.png","key":"filippopedrazzinfp"}],"frontMatter":{"slug":"serving-falcon-7b-fastapi-docker","title":"Serving Falcon 7B Instruct with FastAPI and Docker","authors":["filippopedrazzinfp"],"tags":["llm","self-hosted","prem","open-source","fastapi","docker","falcon-7b"],"description":"In this tutorial, we will walk you through the process of serving the Falcon 7B Instruction model using FastAPI and Docker. The complete code for this tutorial is available on GitHub.","image":"./banner.jpg"},"prevItem":{"title":"Talk to your Data with ChainLit and Langchain","permalink":"/blog/chainlit-langchain-prem"},"nextItem":{"title":"Build a Perplexity AI clone on Prem","permalink":"/blog/perplexity-ai-self-hosted"}},"content":"\x3c!--truncate--\x3e\\n![Prem Banner](./banner.jpg)\\n\\n<head>\\n  <meta name=\\"twitter:image\\" content=\\"./banner.jpg\\"/>\\n</head>\\n\\nIn this tutorial, we will walk you through the process of serving the Falcon 7B Instruction model using FastAPI and Docker. The complete code for this tutorial is available on [GitHub](https://github.com/premAI-io/llm-fastapi-docker-template).\\n\\n> NOTE: in order to run Falcon 7B Instruct model you will need a GPU with at least 16GiB of VRAM. You can use a [Paperspace Cloud](https://www.paperspace.com/gpu-cloud) virtual server or any other cloud provider or your own server with a NVIDIA GPU.\\n\\n##### If you want to just use the model for inference directly, you can use our pre-built docker image like this:\\n\\n```bash\\ndocker run --gpus all -p 8000:8000 ghcr.io/premai-io/chat-falcon-7b-instruct-gpu:latest\\n```\\nThis will ensure that the container has access to the GPU and will expose the API on port 8000. [Learn more](https://github.com/premAI-io/prem-registry/blob/dev/chat-falcon-7b-instruct/README.md).\\n\\n### Step 1: Setup the Python Server\\n\\nFirst, we need to create a requirements.txt file to list all the necessary dependencies. This file will include libraries such as FastAPI, uvicorn, pytest, requests, tqdm, httpx, python-dotenv, tenacity, einops, sentencepiece, accelerate, and xformers.\\n\\n#### 1. Create `requirements.txt` file.\\n\\nCreate a `requirements.txt` file with the following dependencies:\\n\\n```txt\\nfastapi==0.95.0\\nuvicorn==0.21.1\\npytest==7.2.2\\nrequests==2.28.2\\ntqdm==4.65.0\\nhttpx==0.23.3\\npython-dotenv==1.0.0\\ntenacity==8.2.2\\neinops==0.6.1\\nsentencepiece==0.1.99\\naccelerate>=0.16.0,<1\\nxformers==0.0.20\\n```\\n\\n### Step 2: Expose Falcon 7B Instruct with FastAPI\\n\\nNext, we will create a `models.py` file to define the model class that will be used to serve the Falcon 7B Instruction model. We will use the `transformers` library to fetch the model from the HuggingFace Hub.\\n\\nWe will also need a `utils.py` file to define the stopping criteria for the Falcon model. This criteria is used to signal the model when to stop generating new tokens.\\n\\nFinally, we will create a `routes.py` file to define the endpoints that our FastAPI web server will handle. This file will include the logic for generating responses and handling exceptions.\\n\\n#### 1. Create `models.py` file.\\n\\n```python\\nimport os\\nimport torch\\n\\nfrom typing import List\\nfrom transformers import AutoTokenizer, Pipeline, pipeline\\n\\nclass FalconBasedModel(object):\\n    model = None\\n    stopping_criteria = None\\n\\n    @classmethod\\n    def generate(\\n        cls,\\n        messages: list,\\n        temperature: float = 0.9,\\n        top_p: float = 0.9,\\n        n: int = 1,\\n        stream: bool = False,\\n        max_tokens: int = 256,\\n        stop: str = \\"\\",\\n        **kwargs,\\n    ) -> List:\\n        message = messages[-1][\\"content\\"]\\n        return [\\n            cls.model(\\n                message,\\n                max_length=max_tokens,\\n                num_return_sequences=n,\\n                temperature=temperature,\\n                top_p=top_p,\\n                eos_token_id=cls.tokenizer.eos_token_id,\\n                return_full_text=kwargs.get(\\"return_full_text\\", False),\\n                do_sample=kwargs.get(\\"do_sample\\", True),\\n                stop_sequence=stop[0] if stop else None,\\n                stopping_criteria=cls.stopping_criteria(stop, message, cls.tokenizer),\\n            )[0][\\"generated_text\\"]\\n        ]\\n\\n    @classmethod\\n    def get_model(cls) -> Pipeline:\\n        if cls.model is None:\\n            cls.tokenizer = AutoTokenizer.from_pretrained(\\n                os.getenv(\\"MODEL_ID\\", \\"tiiuae/falcon-7b-instruct\\"),\\n                trust_remote_code=True,\\n            )\\n            cls.model = pipeline(\\n                tokenizer=cls.tokenizer,\\n                model=os.getenv(\\"MODEL_ID\\", \\"tiiuae/falcon-7b-instruct\\"),\\n                torch_dtype=torch.bfloat16,\\n                trust_remote_code=True,\\n                device_map=os.getenv(\\"DEVICE\\", \\"auto\\"),\\n            )\\n        cls.stopping_criteria = FalconStoppingCriteria\\n        return cls.model\\n\\n```\\n\\nIn the provided code, a class named `FalconBasedModel` is used to encapsulate the functionality related to the Falcon 7B Instruction model. This class-based approach has several advantages:\\n\\n1. **Encapsulation**: By using a class, we can bundle together the model, its tokenizer, and its stopping criteria into a single unit. This makes the code more organized and easier to understand. It also allows us to hide the internal details of how the model works, exposing only the methods that are necessary for interacting with it.\\n\\n2. **State Preservation**: Class methods can access and modify the state of an instance of the class. In this case, the `FalconBasedModel` class maintains the state of the model and its tokenizer. This is useful because it allows us to load the model and tokenizer only once, when the `get_model` method is first called, and then reuse them for subsequent calls to the `generate` method. This can significantly improve performance, as loading a model and tokenizer can be computationally expensive operations.\\n\\n#### 2. Create `utils.py` file.\\n\\nLike other generative AI models, Falcon requires a stopping criteria to determine when to cease generating new tokens. We will use a straightforward stopping criteria that checks if the target sequence is present in the generated text. The default value is set to \'User:\', but developers can provide a custom target sequence through APIs.\\n\\n```python\\n\\nfrom typing import List\\n\\nfrom transformers import StoppingCriteria\\n\\nclass FalconStoppingCriteria(StoppingCriteria):\\n    def __init__(self, target_sequences: List[str], prompt, tokenizer) -> None:\\n        self.target_sequences = target_sequences\\n        self.prompt = prompt\\n        self.tokenizer = tokenizer\\n\\n    def __call__(self, input_ids, scores, **kwargs) -> bool:\\n        if not self.target_sequences:\\n            return False\\n        # Get the generated text as a string\\n        generated_text = self.tokenizer.decode(input_ids[0])\\n        generated_text = generated_text.replace(self.prompt, \\"\\")\\n        # Check if the target sequence appears in the generated text\\n        return any(\\n            target_sequence in generated_text\\n            for target_sequence in self.target_sequences\\n        )\\n\\n    def __len__(self) -> int:\\n        return len(self.target_sequences)\\n\\n    def __iter__(self):\\n        yield self\\n```\\n\\n#### 3. Create `routes.py` file.\\n\\nNext, we\'ll define the endpoints that our FastAPI web server will handle.\\n\\n```python\\nimport json\\nimport os\\nimport uuid\\nfrom datetime import datetime as dt\\nfrom typing import Any, Dict, Generator, List, Optional, Union\\n\\nfrom fastapi import APIRouter, HTTPException\\nfrom fastapi.responses import StreamingResponse\\nfrom pydantic import BaseModel\\n\\nfrom models import FalconBasedModel as model\\n\\n\\nclass ChatCompletionInput(BaseModel):\\n    model: str\\n    messages: List[dict]\\n    temperature: float = 1.0\\n    top_p: float = 1.0\\n    n: int = 1\\n    stream: bool = False\\n    stop: Optional[Union[str, List[str]]] = [\\"User:\\"]\\n    max_tokens: int = 64\\n    presence_penalty: float = 0.0\\n    frequence_penalty: float = 0.0\\n    logit_bias: Optional[dict] = {}\\n    user: str = \\"\\"\\n\\n\\nclass ChatCompletionResponse(BaseModel):\\n    id: str = uuid.uuid4()\\n    model: str\\n    object: str = \\"chat.completion\\"\\n    created: int = int(dt.now().timestamp())\\n    choices: List[dict]\\n    usage: dict = {\\"prompt_tokens\\": 0, \\"completion_tokens\\": 0, \\"total_tokens\\": 0}\\n\\n\\nrouter = APIRouter()\\n\\n\\nasync def generate_chunk_based_response(body, text) -> Generator[str, Any, None]:\\n    yield \\"event: completion\\\\ndata: \\" + json.dumps(\\n        {\\n            \\"id\\": str(uuid.uuid4()),\\n            \\"model\\": body.model,\\n            \\"object\\": \\"chat.completion\\",\\n            \\"choices\\": [\\n                {\\n                    \\"role\\": \\"assistant\\",\\n                    \\"index\\": 1,\\n                    \\"delta\\": {\\"role\\": \\"assistant\\", \\"content\\": text},\\n                    \\"finish_reason\\": \\"stop\\",\\n                }\\n            ],\\n            \\"usage\\": {\\"prompt_tokens\\": 0, \\"completion_tokens\\": 0, \\"total_tokens\\": 0},\\n        }\\n    ) + \\"\\\\n\\\\n\\"\\n    yield \\"event: done\\\\ndata: [DONE]\\\\n\\\\n\\"\\n\\n\\n@router.post(\\"/chat/completions\\", response_model=ChatCompletionResponse)\\nasync def chat_completions(body: ChatCompletionInput) -> Dict[str, Any]:\\n    try:\\n        predictions = model.generate(\\n            messages=body.messages,\\n            temperature=body.temperature,\\n            top_p=body.top_p,\\n            n=body.n,\\n            stream=body.stream,\\n            max_tokens=body.max_tokens,\\n            stop=body.stop,\\n        )\\n        if body.stream:\\n            return StreamingResponse(\\n                generate_chunk_based_response(body, predictions[0]),\\n                media_type=\\"text/event-stream\\",\\n            )\\n        return ChatCompletionResponse(\\n            id=str(uuid.uuid4()),\\n            model=os.getenv(\\"MODEL_ID\\", \\"tiiuae/falcon-7b-instruct\\"),\\n            object=\\"chat.completion\\",\\n            created=int(dt.now().timestamp()),\\n            choices=[\\n                {\\n                    \\"role\\": \\"assistant\\",\\n                    \\"index\\": idx,\\n                    \\"message\\": {\\"role\\": \\"assistant\\", \\"content\\": text},\\n                    \\"finish_reason\\": \\"stop\\",\\n                }\\n                for idx, text in enumerate(predictions)\\n            ],\\n            usage={\\"prompt_tokens\\": 0, \\"completion_tokens\\": 0, \\"total_tokens\\": 0},\\n        )\\n    except ValueError as error:\\n        raise HTTPException(\\n            status_code=400,\\n            detail={\\"message\\": str(error)},\\n        )\\n\\n```\\n\\n#### 4. Create `main.py` file.\\n\\nThe `main.py` file is the entry point for our FastAPI application. It is responsible for setting up the application and starting the server.\\n\\nOne important aspect of this file is the `create_start_app_handler` function. This function is designed to be called when the FastAPI application starts up. It creates a function, `start_app`, that is responsible for loading the Falcon 7B Instruction model into memory. This is done by calling the `get_model` method of the `FalconBasedModel` class.\\n\\nThe reason we load the model into memory at startup is to improve the performance of our application. Loading a model is a time-consuming operation. If we were to load the model every time we needed to use it, it would significantly slow down our application. By loading the model at startup, we ensure that it\'s done only once, no matter how many requests our application needs to handle.\\n\\nThe `start_app` function is then returned and registered as a startup event handler for our FastAPI application. This means that FastAPI will automatically call this function when the application starts up, ensuring that our model is loaded and ready to use.\\n\\nThe rest of the `main.py` file is responsible for setting up the FastAPI application, including registering our API routes and setting up CORS (Cross-Origin Resource Sharing) middleware. Finally, if this file is run directly (i.e., it is the main module), it starts the FastAPI server using uvicorn.\\n\\n```python\\nimport logging\\nfrom typing import Callable\\n\\nimport uvicorn\\nfrom dotenv import load_dotenv\\nfrom fastapi import FastAPI\\nfrom fastapi.middleware.cors import CORSMiddleware\\nfrom routes import router as api_router\\n\\nfrom models import FalconBasedModel\\n\\ndef create_start_app_handler(app: FastAPI) -> Callable[[], None]:\\n    def start_app() -> None:\\n        FalconBasedModel.get_model()\\n\\n    return start_app\\n\\n\\ndef get_application() -> FastAPI:\\n    application = FastAPI(title=\\"prem-chat-falcon\\", debug=True, version=\\"0.0.1\\")\\n    application.include_router(api_router, prefix=\\"/v1\\")\\n    application.add_event_handler(\\"startup\\", create_start_app_handler(application))\\n    application.add_middleware(\\n        CORSMiddleware,\\n        allow_origins=[\\"*\\"],\\n        allow_credentials=True,\\n        allow_methods=[\\"*\\"],\\n        allow_headers=[\\"*\\"],\\n    )\\n    return application\\n\\n\\napp = get_application()\\n\\n\\nif __name__ == \\"__main__\\":\\n    uvicorn.run(\\"main:app\\", host=\\"0.0.0.0\\", port=8000)\\n\\n```\\n\\n### Step 3: Use Docker to build and run the application\\n\\nTo build and run the application, we will first create a `download.py` file. This script will be called at build time to download the model and cache it in the Docker image.\\n\\nNext, we will create a Dockerfile that uses the official image from HuggingFace, which includes all the necessary dependencies. This Dockerfile will define the steps to build our Docker image.\\n\\nTo avoid including any unused files in the build process, we will also create a `.dockerignore` file.\\n\\n#### 1. Create a `download.py` file.\\n\\nThe download script will be called at build time to download the model and cache it in the Docker image.\\n\\n```python\\nimport argparse\\nimport os\\n\\nimport torch\\nimport transformers\\nfrom tenacity import retry, stop_after_attempt, wait_fixed\\nfrom transformers import AutoTokenizer\\n\\nparser = argparse.ArgumentParser()\\nparser.add_argument(\\"--model\\", help=\\"Model to download\\")\\nargs = parser.parse_args()\\n\\nprint(f\\"Downloading model {args.model}\\")\\n\\n\\n@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))\\ndef download_model() -> None:\\n    _ = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)\\n    _ = transformers.pipeline(\\n        model=args.model,\\n        torch_dtype=torch.bfloat16,\\n        trust_remote_code=True,\\n        device_map=os.getenv(\\"DEVICE\\", \\"auto\\"),\\n    )\\n\\n\\ndownload_model()\\n\\n```\\n\\n#### 2. Create a `Dockerfile`.\\n\\n```dockerfile\\nFROM huggingface/transformers-pytorch-gpu:4.28.1\\n\\nARG MODEL_ID\\n\\nWORKDIR /usr/src/app/\\n\\nCOPY requirements.txt ./\\n\\nRUN pip install --no-cache-dir -r ./requirements.txt --upgrade pip\\n\\nCOPY download.py .\\n\\nRUN python3 download.py --model $MODEL_ID\\n\\nCOPY . .\\n\\nENV MODEL_ID=$MODEL_ID\\n\\nCMD python3 main.py\\n```\\n\\n#### 4. Create a `.dockerignore` file.\\n\\n```dockerfile\\n.editorconfig\\n.gitattributes\\n.github\\n.gitignore\\n.gitlab-ci.yml\\n.idea\\n.pre-commit-config.yaml\\n.readthedocs.yml\\n.travis.yml\\nvenv\\n.git\\n./ml/models/\\n.bin\\n```\\n\\n### Step 4: Build and run the application\\n\\nFinally, we will build the Docker image using the `docker build` command and run it using the `docker run` command.\\n\\n#### 1. Build the Docker image.\\n\\n```bash\\ndocker build --file ./Dockerfile \\\\\\n    --build-arg=\\"MODEL_ID=tiiuae/falcon-7b-instruct\\" \\\\\\n    --tag blog-post/chat-falcon-7b-instruct-gpu:latest \\\\\\n    --tag blog-post/chat-falcon-7b-instruct-gpu:0.0.1 \\\\\\n    .\\n```\\n\\n#### 2. Run the Docker image.\\n\\n```bash\\ndocker run --gpus all -p 8000:8000 blog-post/chat-falcon-7b-instruct-gpu:latest\\n```\\n\\n### Conclusion\\n\\nIn this tutorial, we have demonstrated how to serve the Falcon 7B Instruction model using FastAPI and Docker. This is a crucial first step in serving a model for production use cases."},{"id":"perplexity-ai-self-hosted","metadata":{"permalink":"/blog/perplexity-ai-self-hosted","editUrl":"https://github.com/premAI-io/dev-portal/blob/main/blog/2023-07-01-perplexity-ai-self-hosted/index.md","source":"@site/blog/2023-07-01-perplexity-ai-self-hosted/index.md","title":"Build a Perplexity AI clone on Prem","description":"Build your own Perplexity AI clone with Prem using the `Dolly v2 12B` model, self-hosted on Paperspace Cloud virtual server.","date":"2023-07-01T00:00:00.000Z","formattedDate":"July 1, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"ai","permalink":"/blog/tags/ai"},{"label":"self-hosted","permalink":"/blog/tags/self-hosted"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"open-source","permalink":"/blog/tags/open-source"},{"label":"perplexity","permalink":"/blog/tags/perplexity"},{"label":"paperspace","permalink":"/blog/tags/paperspace"},{"label":"dolly","permalink":"/blog/tags/dolly"}],"readingTime":0.385,"hasTruncateMarker":true,"authors":[{"name":"Marco Argentieri","title":"Bitcoin wizard","url":"https://github.com/tiero","imageURL":"https://github.com/tiero.png","key":"tiero"}],"frontMatter":{"slug":"perplexity-ai-self-hosted","title":"Build a Perplexity AI clone on Prem","authors":["tiero"],"tags":["llm","ai","self-hosted","prem","open-source","perplexity","paperspace","dolly"],"description":"Build your own Perplexity AI clone with Prem using the `Dolly v2 12B` model, self-hosted on Paperspace Cloud virtual server.","image":"./screenshot.png"},"prevItem":{"title":"Serving Falcon 7B Instruct with FastAPI and Docker","permalink":"/blog/serving-falcon-7b-fastapi-docker"},"nextItem":{"title":"Hello Prem!","permalink":"/blog/hello-prem"}},"content":"\x3c!--truncate--\x3e\\n\\n<head>\\n  <meta name=\\"twitter:image\\" content=\\"./screenshot.png\\"/>\\n</head>\\n\\n> We have been working on a new Developer Platform aimed at enabling companies to transition from closed-source models, like those from OpenAI, to open-source alternatives such as Mistral and Llama. Our platform offers all the necessary tools (evaluation, fine-tuning, and monitoring) to make this switch with confidence. You can check it out here: https://app.premai.io. Additionally, we have launched a new Generative AI Startup Grant Program. To learn more, visit: https://blog.premai.io/announcing-our-startup-grants-program/."},{"id":"hello-prem","metadata":{"permalink":"/blog/hello-prem","editUrl":"https://github.com/premAI-io/dev-portal/blob/main/blog/2023-06-26-welcome/index.md","source":"@site/blog/2023-06-26-welcome/index.md","title":"Hello Prem!","description":"Hello, I am Filippo and I am currently contributing to Prem.","date":"2023-06-26T00:00:00.000Z","formattedDate":"June 26, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"self-hosted","permalink":"/blog/tags/self-hosted"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"open-source","permalink":"/blog/tags/open-source"},{"label":"welcome","permalink":"/blog/tags/welcome"}],"readingTime":0.54,"hasTruncateMarker":true,"authors":[{"name":"Filippo Pedrazzini","title":"Core contributor @ PremAI","url":"https://github.com/filopedraz","imageURL":"https://github.com/filopedraz.png","key":"filippopedrazzinfp"}],"frontMatter":{"slug":"hello-prem","title":"Hello Prem!","authors":["filippopedrazzinfp"],"tags":["llm","self-hosted","prem","open-source","welcome"],"description":"Hello, I am Filippo and I am currently contributing to Prem.","image":"./banner.png"},"prevItem":{"title":"Build a Perplexity AI clone on Prem","permalink":"/blog/perplexity-ai-self-hosted"}},"content":"\x3c!-- truncate --\x3e\\n![Prem Banner](./banner.png)\\n\\nHello, I am Filippo and I am currently contributing to Prem.\\n\\nWelcome to Prem!\\n\\n> We have been working on a new Developer Platform aimed at enabling companies to transition from closed-source models, like those from OpenAI, to open-source alternatives such as Mistral and Llama. Our platform offers all the necessary tools (evaluation, fine-tuning, and monitoring) to make this switch with confidence. You can check it out here: https://app.premai.io. Additionally, we have launched a new Generative AI Startup Grant Program. To learn more, visit: https://blog.premai.io/announcing-our-startup-grants-program/.\\n\\n- Be part of the community [joining our Discord](https://discord.com/invite/kpKk6vYVAn).\\n- To stay in touch [follow us on Twitter](https://twitter.com/premai_io)."}]}')}}]);